{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "8fd71713",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Installing dependencies from /home/doran/jupyterlab/article-scripts/arize/requirements.txt\n",
            "Collecting arize-phoenix (from -r requirements.txt (line 1))\n",
            "  Downloading arize_phoenix-12.33.1-py3-none-any.whl.metadata (35 kB)\n",
            "Collecting arize-phoenix-otel (from -r requirements.txt (line 2))\n",
            "  Downloading arize_phoenix_otel-0.14.0-py3-none-any.whl.metadata (8.3 kB)\n",
            "Collecting openinference-instrumentation-langchain (from -r requirements.txt (line 3))\n",
            "  Downloading openinference_instrumentation_langchain-0.1.58-py3-none-any.whl.metadata (6.7 kB)\n",
            "Collecting langchain (from -r requirements.txt (line 4))\n",
            "  Downloading langchain-1.2.9-py3-none-any.whl.metadata (5.7 kB)\n",
            "Collecting langchain-openai (from -r requirements.txt (line 5))\n",
            "  Downloading langchain_openai-1.1.7-py3-none-any.whl.metadata (2.6 kB)\n",
            "Collecting langchain-tavily (from -r requirements.txt (line 6))\n",
            "  Downloading langchain_tavily-0.2.17-py3-none-any.whl.metadata (20 kB)\n",
            "Requirement already satisfied: openai in /home/doran/jupyterlab/.venv/lib/python3.12/site-packages (from -r requirements.txt (line 7)) (2.16.0)\n",
            "Requirement already satisfied: python-dotenv in /home/doran/jupyterlab/.venv/lib/python3.12/site-packages (from -r requirements.txt (line 8)) (1.2.1)\n",
            "Collecting jupyter (from -r requirements.txt (line 9))\n",
            "  Downloading jupyter-1.1.1-py2.py3-none-any.whl.metadata (2.0 kB)\n",
            "Collecting aioitertools (from arize-phoenix->-r requirements.txt (line 1))\n",
            "  Downloading aioitertools-0.13.0-py3-none-any.whl.metadata (3.3 kB)\n",
            "Collecting aiosqlite (from arize-phoenix->-r requirements.txt (line 1))\n",
            "  Downloading aiosqlite-0.22.1-py3-none-any.whl.metadata (4.3 kB)\n",
            "Collecting alembic<2,>=1.3.0 (from arize-phoenix->-r requirements.txt (line 1))\n",
            "  Downloading alembic-1.18.3-py3-none-any.whl.metadata (7.2 kB)\n",
            "Collecting arize-phoenix-client>=1.28.0 (from arize-phoenix->-r requirements.txt (line 1))\n",
            "  Downloading arize_phoenix_client-1.28.0-py3-none-any.whl.metadata (11 kB)\n",
            "Collecting arize-phoenix-evals>=2.8.0 (from arize-phoenix->-r requirements.txt (line 1))\n",
            "  Downloading arize_phoenix_evals-2.9.0-py3-none-any.whl.metadata (6.6 kB)\n",
            "Collecting authlib (from arize-phoenix->-r requirements.txt (line 1))\n",
            "  Downloading authlib-1.6.7-py2.py3-none-any.whl.metadata (9.8 kB)\n",
            "Collecting cachetools (from arize-phoenix->-r requirements.txt (line 1))\n",
            "  Downloading cachetools-7.0.0-py3-none-any.whl.metadata (5.6 kB)\n",
            "Collecting email-validator (from arize-phoenix->-r requirements.txt (line 1))\n",
            "  Downloading email_validator-2.3.0-py3-none-any.whl.metadata (26 kB)\n",
            "Requirement already satisfied: fastapi in /home/doran/jupyterlab/.venv/lib/python3.12/site-packages (from arize-phoenix->-r requirements.txt (line 1)) (0.128.0)\n",
            "Collecting grpc-interceptor (from arize-phoenix->-r requirements.txt (line 1))\n",
            "  Downloading grpc_interceptor-0.15.4-py3-none-any.whl.metadata (8.4 kB)\n",
            "Requirement already satisfied: grpcio in /home/doran/jupyterlab/.venv/lib/python3.12/site-packages (from arize-phoenix->-r requirements.txt (line 1)) (1.76.0)\n",
            "Requirement already satisfied: httpx in /home/doran/jupyterlab/.venv/lib/python3.12/site-packages (from arize-phoenix->-r requirements.txt (line 1)) (0.28.1)\n",
            "Requirement already satisfied: jinja2 in /home/doran/jupyterlab/.venv/lib/python3.12/site-packages (from arize-phoenix->-r requirements.txt (line 1)) (3.1.6)\n",
            "Collecting jmespath (from arize-phoenix->-r requirements.txt (line 1))\n",
            "  Downloading jmespath-1.1.0-py3-none-any.whl.metadata (7.6 kB)\n",
            "Collecting ldap3 (from arize-phoenix->-r requirements.txt (line 1))\n",
            "  Downloading ldap3-2.9.1-py2.py3-none-any.whl.metadata (5.4 kB)\n",
            "Requirement already satisfied: numpy!=2.0.0 in /home/doran/jupyterlab/.venv/lib/python3.12/site-packages (from arize-phoenix->-r requirements.txt (line 1)) (2.2.6)\n",
            "Collecting openinference-instrumentation>=0.1.32 (from arize-phoenix->-r requirements.txt (line 1))\n",
            "  Downloading openinference_instrumentation-0.1.44-py3-none-any.whl.metadata (5.0 kB)\n",
            "Collecting openinference-semantic-conventions>=0.1.20 (from arize-phoenix->-r requirements.txt (line 1))\n",
            "  Downloading openinference_semantic_conventions-0.1.26-py3-none-any.whl.metadata (1.3 kB)\n",
            "Collecting opentelemetry-exporter-otlp (from arize-phoenix->-r requirements.txt (line 1))\n",
            "  Downloading opentelemetry_exporter_otlp-1.39.1-py3-none-any.whl.metadata (2.4 kB)\n",
            "Collecting opentelemetry-proto>=1.12.0 (from arize-phoenix->-r requirements.txt (line 1))\n",
            "  Downloading opentelemetry_proto-1.39.1-py3-none-any.whl.metadata (2.3 kB)\n",
            "Collecting opentelemetry-sdk (from arize-phoenix->-r requirements.txt (line 1))\n",
            "  Downloading opentelemetry_sdk-1.39.1-py3-none-any.whl.metadata (1.5 kB)\n",
            "Collecting opentelemetry-semantic-conventions (from arize-phoenix->-r requirements.txt (line 1))\n",
            "  Downloading opentelemetry_semantic_conventions-0.60b1-py3-none-any.whl.metadata (2.4 kB)\n",
            "Requirement already satisfied: orjson in /home/doran/jupyterlab/.venv/lib/python3.12/site-packages (from arize-phoenix->-r requirements.txt (line 1)) (3.11.6)\n",
            "Requirement already satisfied: pandas>=1.0 in /home/doran/jupyterlab/.venv/lib/python3.12/site-packages (from arize-phoenix->-r requirements.txt (line 1)) (2.3.2)\n",
            "Requirement already satisfied: prometheus-client in /home/doran/jupyterlab/.venv/lib/python3.12/site-packages (from arize-phoenix->-r requirements.txt (line 1)) (0.24.1)\n",
            "Requirement already satisfied: protobuf>=4.25.8 in /home/doran/jupyterlab/.venv/lib/python3.12/site-packages (from arize-phoenix->-r requirements.txt (line 1)) (5.29.5)\n",
            "Requirement already satisfied: psutil in /home/doran/jupyterlab/.venv/lib/python3.12/site-packages (from arize-phoenix->-r requirements.txt (line 1)) (7.2.1)\n",
            "Requirement already satisfied: pyarrow in /home/doran/jupyterlab/.venv/lib/python3.12/site-packages (from arize-phoenix->-r requirements.txt (line 1)) (22.0.0)\n",
            "Requirement already satisfied: pydantic>=2.1.0 in /home/doran/jupyterlab/.venv/lib/python3.12/site-packages (from arize-phoenix->-r requirements.txt (line 1)) (2.12.5)\n",
            "Requirement already satisfied: python-dateutil in /home/doran/jupyterlab/.venv/lib/python3.12/site-packages (from arize-phoenix->-r requirements.txt (line 1)) (2.9.0.post0)\n",
            "Requirement already satisfied: python-multipart in /home/doran/jupyterlab/.venv/lib/python3.12/site-packages (from arize-phoenix->-r requirements.txt (line 1)) (0.0.22)\n",
            "Requirement already satisfied: scikit-learn in /home/doran/jupyterlab/.venv/lib/python3.12/site-packages (from arize-phoenix->-r requirements.txt (line 1)) (1.7.1)\n",
            "Requirement already satisfied: scipy in /home/doran/jupyterlab/.venv/lib/python3.12/site-packages (from arize-phoenix->-r requirements.txt (line 1)) (1.17.0)\n",
            "Collecting sqlalchemy<3,>=2.0.4 (from sqlalchemy[asyncio]<3,>=2.0.4->arize-phoenix->-r requirements.txt (line 1))\n",
            "  Downloading sqlalchemy-2.0.46-cp312-cp312-manylinux2014_aarch64.manylinux_2_17_aarch64.manylinux_2_28_aarch64.whl.metadata (9.5 kB)\n",
            "Collecting sqlean-py>=3.45.1 (from arize-phoenix->-r requirements.txt (line 1))\n",
            "  Downloading sqlean_py-3.50.4.5-cp312-cp312-manylinux_2_28_aarch64.whl.metadata (4.5 kB)\n",
            "Requirement already satisfied: starlette in /home/doran/jupyterlab/.venv/lib/python3.12/site-packages (from arize-phoenix->-r requirements.txt (line 1)) (0.50.0)\n",
            "Collecting strawberry-graphql==0.287.3 (from arize-phoenix->-r requirements.txt (line 1))\n",
            "  Downloading strawberry_graphql-0.287.3-py3-none-any.whl.metadata (7.6 kB)\n",
            "Requirement already satisfied: tqdm in /home/doran/jupyterlab/.venv/lib/python3.12/site-packages (from arize-phoenix->-r requirements.txt (line 1)) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions>=4.6 in /home/doran/jupyterlab/.venv/lib/python3.12/site-packages (from arize-phoenix->-r requirements.txt (line 1)) (4.15.0)\n",
            "Requirement already satisfied: uvicorn in /home/doran/jupyterlab/.venv/lib/python3.12/site-packages (from arize-phoenix->-r requirements.txt (line 1)) (0.40.0)\n",
            "Collecting wrapt<2,>=1.17.2 (from arize-phoenix->-r requirements.txt (line 1))\n",
            "  Downloading wrapt-1.17.3-cp312-cp312-manylinux2014_aarch64.manylinux_2_17_aarch64.manylinux_2_28_aarch64.whl.metadata (6.4 kB)\n",
            "Collecting graphql-core<3.4.0,>=3.2.0 (from strawberry-graphql==0.287.3->arize-phoenix->-r requirements.txt (line 1))\n",
            "  Downloading graphql_core-3.2.7-py3-none-any.whl.metadata (11 kB)\n",
            "Collecting lia-web>=0.2.1 (from strawberry-graphql==0.287.3->arize-phoenix->-r requirements.txt (line 1))\n",
            "  Downloading lia_web-0.3.1-py3-none-any.whl.metadata (1.2 kB)\n",
            "Requirement already satisfied: packaging>=23 in /home/doran/jupyterlab/.venv/lib/python3.12/site-packages (from strawberry-graphql==0.287.3->arize-phoenix->-r requirements.txt (line 1)) (25.0)\n",
            "Collecting opentelemetry-api (from openinference-instrumentation-langchain->-r requirements.txt (line 3))\n",
            "  Downloading opentelemetry_api-1.39.1-py3-none-any.whl.metadata (1.5 kB)\n",
            "Collecting opentelemetry-instrumentation (from openinference-instrumentation-langchain->-r requirements.txt (line 3))\n",
            "  Downloading opentelemetry_instrumentation-0.60b1-py3-none-any.whl.metadata (7.2 kB)\n",
            "Collecting langchain-core<2.0.0,>=1.2.9 (from langchain->-r requirements.txt (line 4))\n",
            "  Downloading langchain_core-1.2.9-py3-none-any.whl.metadata (4.4 kB)\n",
            "Collecting langgraph<1.1.0,>=1.0.7 (from langchain->-r requirements.txt (line 4))\n",
            "  Downloading langgraph-1.0.8-py3-none-any.whl.metadata (7.4 kB)\n",
            "Requirement already satisfied: tiktoken<1.0.0,>=0.7.0 in /home/doran/jupyterlab/.venv/lib/python3.12/site-packages (from langchain-openai->-r requirements.txt (line 5)) (0.12.0)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.11.14 in /home/doran/jupyterlab/.venv/lib/python3.12/site-packages (from langchain-tavily->-r requirements.txt (line 6)) (3.11.18)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.32.3 in /home/doran/jupyterlab/.venv/lib/python3.12/site-packages (from langchain-tavily->-r requirements.txt (line 6)) (2.32.5)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /home/doran/jupyterlab/.venv/lib/python3.12/site-packages (from openai->-r requirements.txt (line 7)) (4.12.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /home/doran/jupyterlab/.venv/lib/python3.12/site-packages (from openai->-r requirements.txt (line 7)) (1.9.0)\n",
            "Requirement already satisfied: jiter<1,>=0.10.0 in /home/doran/jupyterlab/.venv/lib/python3.12/site-packages (from openai->-r requirements.txt (line 7)) (0.12.0)\n",
            "Requirement already satisfied: sniffio in /home/doran/jupyterlab/.venv/lib/python3.12/site-packages (from openai->-r requirements.txt (line 7)) (1.3.1)\n",
            "Collecting notebook (from jupyter->-r requirements.txt (line 9))\n",
            "  Downloading notebook-7.5.3-py3-none-any.whl.metadata (10 kB)\n",
            "Collecting jupyter-console (from jupyter->-r requirements.txt (line 9))\n",
            "  Downloading jupyter_console-6.6.3-py3-none-any.whl.metadata (5.8 kB)\n",
            "Requirement already satisfied: nbconvert in /home/doran/jupyterlab/.venv/lib/python3.12/site-packages (from jupyter->-r requirements.txt (line 9)) (7.16.6)\n",
            "Requirement already satisfied: ipykernel in /home/doran/jupyterlab/.venv/lib/python3.12/site-packages (from jupyter->-r requirements.txt (line 9)) (6.31.0)\n",
            "Requirement already satisfied: ipywidgets in /home/doran/jupyterlab/.venv/lib/python3.12/site-packages (from jupyter->-r requirements.txt (line 9)) (8.1.7)\n",
            "Requirement already satisfied: jupyterlab in /home/doran/jupyterlab/.venv/lib/python3.12/site-packages (from jupyter->-r requirements.txt (line 9)) (4.4.7)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /home/doran/jupyterlab/.venv/lib/python3.12/site-packages (from aiohttp<4.0.0,>=3.11.14->langchain-tavily->-r requirements.txt (line 6)) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /home/doran/jupyterlab/.venv/lib/python3.12/site-packages (from aiohttp<4.0.0,>=3.11.14->langchain-tavily->-r requirements.txt (line 6)) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /home/doran/jupyterlab/.venv/lib/python3.12/site-packages (from aiohttp<4.0.0,>=3.11.14->langchain-tavily->-r requirements.txt (line 6)) (25.4.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /home/doran/jupyterlab/.venv/lib/python3.12/site-packages (from aiohttp<4.0.0,>=3.11.14->langchain-tavily->-r requirements.txt (line 6)) (1.8.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /home/doran/jupyterlab/.venv/lib/python3.12/site-packages (from aiohttp<4.0.0,>=3.11.14->langchain-tavily->-r requirements.txt (line 6)) (6.7.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /home/doran/jupyterlab/.venv/lib/python3.12/site-packages (from aiohttp<4.0.0,>=3.11.14->langchain-tavily->-r requirements.txt (line 6)) (0.4.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /home/doran/jupyterlab/.venv/lib/python3.12/site-packages (from aiohttp<4.0.0,>=3.11.14->langchain-tavily->-r requirements.txt (line 6)) (1.22.0)\n",
            "Collecting Mako (from alembic<2,>=1.3.0->arize-phoenix->-r requirements.txt (line 1))\n",
            "  Downloading mako-1.3.10-py3-none-any.whl.metadata (2.9 kB)\n",
            "Requirement already satisfied: idna>=2.8 in /home/doran/jupyterlab/.venv/lib/python3.12/site-packages (from anyio<5,>=3.5.0->openai->-r requirements.txt (line 7)) (3.11)\n",
            "Collecting jsonpath-ng (from arize-phoenix-evals>=2.8.0->arize-phoenix->-r requirements.txt (line 1))\n",
            "  Downloading jsonpath_ng-1.7.0-py3-none-any.whl.metadata (18 kB)\n",
            "Collecting pystache (from arize-phoenix-evals>=2.8.0->arize-phoenix->-r requirements.txt (line 1))\n",
            "  Downloading pystache-0.6.8-py3-none-any.whl.metadata (14 kB)\n",
            "Requirement already satisfied: certifi in /home/doran/jupyterlab/.venv/lib/python3.12/site-packages (from httpx->arize-phoenix->-r requirements.txt (line 1)) (2026.1.4)\n",
            "Requirement already satisfied: httpcore==1.* in /home/doran/jupyterlab/.venv/lib/python3.12/site-packages (from httpx->arize-phoenix->-r requirements.txt (line 1)) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /home/doran/jupyterlab/.venv/lib/python3.12/site-packages (from httpcore==1.*->httpx->arize-phoenix->-r requirements.txt (line 1)) (0.16.0)\n",
            "Collecting jsonpatch<2.0.0,>=1.33.0 (from langchain-core<2.0.0,>=1.2.9->langchain->-r requirements.txt (line 4))\n",
            "  Downloading jsonpatch-1.33-py2.py3-none-any.whl.metadata (3.0 kB)\n",
            "Collecting langsmith<1.0.0,>=0.3.45 (from langchain-core<2.0.0,>=1.2.9->langchain->-r requirements.txt (line 4))\n",
            "  Downloading langsmith-0.6.9-py3-none-any.whl.metadata (15 kB)\n",
            "Requirement already satisfied: pyyaml<7.0.0,>=5.3.0 in /home/doran/jupyterlab/.venv/lib/python3.12/site-packages (from langchain-core<2.0.0,>=1.2.9->langchain->-r requirements.txt (line 4)) (6.0.3)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /home/doran/jupyterlab/.venv/lib/python3.12/site-packages (from langchain-core<2.0.0,>=1.2.9->langchain->-r requirements.txt (line 4)) (9.1.2)\n",
            "Collecting uuid-utils<1.0,>=0.12.0 (from langchain-core<2.0.0,>=1.2.9->langchain->-r requirements.txt (line 4))\n",
            "  Downloading uuid_utils-0.14.0-cp39-abi3-manylinux_2_17_aarch64.manylinux2014_aarch64.whl.metadata (4.9 kB)\n",
            "Collecting langgraph-checkpoint<5.0.0,>=2.1.0 (from langgraph<1.1.0,>=1.0.7->langchain->-r requirements.txt (line 4))\n",
            "  Downloading langgraph_checkpoint-4.0.0-py3-none-any.whl.metadata (4.9 kB)\n",
            "Collecting langgraph-prebuilt<1.1.0,>=1.0.7 (from langgraph<1.1.0,>=1.0.7->langchain->-r requirements.txt (line 4))\n",
            "  Downloading langgraph_prebuilt-1.0.7-py3-none-any.whl.metadata (5.2 kB)\n",
            "Collecting langgraph-sdk<0.4.0,>=0.3.0 (from langgraph<1.1.0,>=1.0.7->langchain->-r requirements.txt (line 4))\n",
            "  Downloading langgraph_sdk-0.3.4-py3-none-any.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: xxhash>=3.5.0 in /home/doran/jupyterlab/.venv/lib/python3.12/site-packages (from langgraph<1.1.0,>=1.0.7->langchain->-r requirements.txt (line 4)) (3.6.0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /home/doran/jupyterlab/.venv/lib/python3.12/site-packages (from pandas>=1.0->arize-phoenix->-r requirements.txt (line 1)) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /home/doran/jupyterlab/.venv/lib/python3.12/site-packages (from pandas>=1.0->arize-phoenix->-r requirements.txt (line 1)) (2025.3)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /home/doran/jupyterlab/.venv/lib/python3.12/site-packages (from pydantic>=2.1.0->arize-phoenix->-r requirements.txt (line 1)) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.41.5 in /home/doran/jupyterlab/.venv/lib/python3.12/site-packages (from pydantic>=2.1.0->arize-phoenix->-r requirements.txt (line 1)) (2.41.5)\n",
            "Requirement already satisfied: typing-inspection>=0.4.2 in /home/doran/jupyterlab/.venv/lib/python3.12/site-packages (from pydantic>=2.1.0->arize-phoenix->-r requirements.txt (line 1)) (0.4.2)\n",
            "Requirement already satisfied: six>=1.5 in /home/doran/jupyterlab/.venv/lib/python3.12/site-packages (from python-dateutil->arize-phoenix->-r requirements.txt (line 1)) (1.17.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /home/doran/jupyterlab/.venv/lib/python3.12/site-packages (from requests<3.0.0,>=2.32.3->langchain-tavily->-r requirements.txt (line 6)) (3.4.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/doran/jupyterlab/.venv/lib/python3.12/site-packages (from requests<3.0.0,>=2.32.3->langchain-tavily->-r requirements.txt (line 6)) (2.6.3)\n",
            "Collecting greenlet>=1 (from sqlalchemy<3,>=2.0.4->sqlalchemy[asyncio]<3,>=2.0.4->arize-phoenix->-r requirements.txt (line 1))\n",
            "  Downloading greenlet-3.3.1-cp312-cp312-manylinux_2_24_aarch64.manylinux_2_28_aarch64.whl.metadata (3.7 kB)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /home/doran/jupyterlab/.venv/lib/python3.12/site-packages (from tiktoken<1.0.0,>=0.7.0->langchain-openai->-r requirements.txt (line 5)) (2026.1.15)\n",
            "Requirement already satisfied: cryptography in /home/doran/jupyterlab/.venv/lib/python3.12/site-packages (from authlib->arize-phoenix->-r requirements.txt (line 1)) (46.0.4)\n",
            "Collecting dnspython>=2.0.0 (from email-validator->arize-phoenix->-r requirements.txt (line 1))\n",
            "  Downloading dnspython-2.8.0-py3-none-any.whl.metadata (5.7 kB)\n",
            "Requirement already satisfied: annotated-doc>=0.0.2 in /home/doran/jupyterlab/.venv/lib/python3.12/site-packages (from fastapi->arize-phoenix->-r requirements.txt (line 1)) (0.0.4)\n",
            "Requirement already satisfied: comm>=0.1.1 in /home/doran/jupyterlab/.venv/lib/python3.12/site-packages (from ipykernel->jupyter->-r requirements.txt (line 9)) (0.2.3)\n",
            "Requirement already satisfied: debugpy>=1.6.5 in /home/doran/jupyterlab/.venv/lib/python3.12/site-packages (from ipykernel->jupyter->-r requirements.txt (line 9)) (1.8.19)\n",
            "Requirement already satisfied: ipython>=7.23.1 in /home/doran/jupyterlab/.venv/lib/python3.12/site-packages (from ipykernel->jupyter->-r requirements.txt (line 9)) (9.9.0)\n",
            "Requirement already satisfied: jupyter-client>=8.0.0 in /home/doran/jupyterlab/.venv/lib/python3.12/site-packages (from ipykernel->jupyter->-r requirements.txt (line 9)) (8.8.0)\n",
            "Requirement already satisfied: jupyter-core!=5.0.*,>=4.12 in /home/doran/jupyterlab/.venv/lib/python3.12/site-packages (from ipykernel->jupyter->-r requirements.txt (line 9)) (5.9.1)\n",
            "Requirement already satisfied: matplotlib-inline>=0.1 in /home/doran/jupyterlab/.venv/lib/python3.12/site-packages (from ipykernel->jupyter->-r requirements.txt (line 9)) (0.2.1)\n",
            "Requirement already satisfied: nest-asyncio>=1.4 in /home/doran/jupyterlab/.venv/lib/python3.12/site-packages (from ipykernel->jupyter->-r requirements.txt (line 9)) (1.6.0)\n",
            "Requirement already satisfied: pyzmq>=25 in /home/doran/jupyterlab/.venv/lib/python3.12/site-packages (from ipykernel->jupyter->-r requirements.txt (line 9)) (27.1.0)\n",
            "Requirement already satisfied: tornado>=6.2 in /home/doran/jupyterlab/.venv/lib/python3.12/site-packages (from ipykernel->jupyter->-r requirements.txt (line 9)) (6.5.4)\n",
            "Requirement already satisfied: traitlets>=5.4.0 in /home/doran/jupyterlab/.venv/lib/python3.12/site-packages (from ipykernel->jupyter->-r requirements.txt (line 9)) (5.14.3)\n",
            "Requirement already satisfied: widgetsnbextension~=4.0.14 in /home/doran/jupyterlab/.venv/lib/python3.12/site-packages (from ipywidgets->jupyter->-r requirements.txt (line 9)) (4.0.15)\n",
            "Requirement already satisfied: jupyterlab_widgets~=3.0.15 in /home/doran/jupyterlab/.venv/lib/python3.12/site-packages (from ipywidgets->jupyter->-r requirements.txt (line 9)) (3.0.16)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /home/doran/jupyterlab/.venv/lib/python3.12/site-packages (from jinja2->arize-phoenix->-r requirements.txt (line 1)) (3.0.3)\n",
            "Requirement already satisfied: prompt-toolkit>=3.0.30 in /home/doran/jupyterlab/.venv/lib/python3.12/site-packages (from jupyter-console->jupyter->-r requirements.txt (line 9)) (3.0.52)\n",
            "Requirement already satisfied: pygments in /home/doran/jupyterlab/.venv/lib/python3.12/site-packages (from jupyter-console->jupyter->-r requirements.txt (line 9)) (2.19.2)\n",
            "Requirement already satisfied: async-lru>=1.0.0 in /home/doran/jupyterlab/.venv/lib/python3.12/site-packages (from jupyterlab->jupyter->-r requirements.txt (line 9)) (2.0.5)\n",
            "Requirement already satisfied: jupyter-lsp>=2.0.0 in /home/doran/jupyterlab/.venv/lib/python3.12/site-packages (from jupyterlab->jupyter->-r requirements.txt (line 9)) (2.3.0)\n",
            "Requirement already satisfied: jupyter-server<3,>=2.4.0 in /home/doran/jupyterlab/.venv/lib/python3.12/site-packages (from jupyterlab->jupyter->-r requirements.txt (line 9)) (2.17.0)\n",
            "Requirement already satisfied: jupyterlab-server<3,>=2.27.1 in /home/doran/jupyterlab/.venv/lib/python3.12/site-packages (from jupyterlab->jupyter->-r requirements.txt (line 9)) (2.28.0)\n",
            "Requirement already satisfied: notebook-shim>=0.2 in /home/doran/jupyterlab/.venv/lib/python3.12/site-packages (from jupyterlab->jupyter->-r requirements.txt (line 9)) (0.2.4)\n",
            "Requirement already satisfied: setuptools>=41.1.0 in /home/doran/jupyterlab/.venv/lib/python3.12/site-packages (from jupyterlab->jupyter->-r requirements.txt (line 9)) (80.10.2)\n",
            "Requirement already satisfied: pyasn1>=0.4.6 in /home/doran/jupyterlab/.venv/lib/python3.12/site-packages (from ldap3->arize-phoenix->-r requirements.txt (line 1)) (0.6.2)\n",
            "Requirement already satisfied: beautifulsoup4 in /home/doran/jupyterlab/.venv/lib/python3.12/site-packages (from nbconvert->jupyter->-r requirements.txt (line 9)) (4.14.3)\n",
            "Requirement already satisfied: bleach!=5.0.0 in /home/doran/jupyterlab/.venv/lib/python3.12/site-packages (from bleach[css]!=5.0.0->nbconvert->jupyter->-r requirements.txt (line 9)) (6.3.0)\n",
            "Requirement already satisfied: defusedxml in /home/doran/jupyterlab/.venv/lib/python3.12/site-packages (from nbconvert->jupyter->-r requirements.txt (line 9)) (0.7.1)\n",
            "Requirement already satisfied: jupyterlab-pygments in /home/doran/jupyterlab/.venv/lib/python3.12/site-packages (from nbconvert->jupyter->-r requirements.txt (line 9)) (0.3.0)\n",
            "Requirement already satisfied: mistune<4,>=2.0.3 in /home/doran/jupyterlab/.venv/lib/python3.12/site-packages (from nbconvert->jupyter->-r requirements.txt (line 9)) (3.2.0)\n",
            "Requirement already satisfied: nbclient>=0.5.0 in /home/doran/jupyterlab/.venv/lib/python3.12/site-packages (from nbconvert->jupyter->-r requirements.txt (line 9)) (0.10.4)\n",
            "Requirement already satisfied: nbformat>=5.7 in /home/doran/jupyterlab/.venv/lib/python3.12/site-packages (from nbconvert->jupyter->-r requirements.txt (line 9)) (5.10.4)\n",
            "Requirement already satisfied: pandocfilters>=1.4.1 in /home/doran/jupyterlab/.venv/lib/python3.12/site-packages (from nbconvert->jupyter->-r requirements.txt (line 9)) (1.5.1)\n",
            "Collecting jupyterlab (from jupyter->-r requirements.txt (line 9))\n",
            "  Downloading jupyterlab-4.5.3-py3-none-any.whl.metadata (16 kB)\n",
            "Requirement already satisfied: importlib-metadata<8.8.0,>=6.0 in /home/doran/jupyterlab/.venv/lib/python3.12/site-packages (from opentelemetry-api->openinference-instrumentation-langchain->-r requirements.txt (line 3)) (8.7.1)\n",
            "Collecting opentelemetry-exporter-otlp-proto-grpc==1.39.1 (from opentelemetry-exporter-otlp->arize-phoenix->-r requirements.txt (line 1))\n",
            "  Downloading opentelemetry_exporter_otlp_proto_grpc-1.39.1-py3-none-any.whl.metadata (2.5 kB)\n",
            "Collecting opentelemetry-exporter-otlp-proto-http==1.39.1 (from opentelemetry-exporter-otlp->arize-phoenix->-r requirements.txt (line 1))\n",
            "  Downloading opentelemetry_exporter_otlp_proto_http-1.39.1-py3-none-any.whl.metadata (2.4 kB)\n",
            "Requirement already satisfied: googleapis-common-protos~=1.57 in /home/doran/jupyterlab/.venv/lib/python3.12/site-packages (from opentelemetry-exporter-otlp-proto-grpc==1.39.1->opentelemetry-exporter-otlp->arize-phoenix->-r requirements.txt (line 1)) (1.72.0)\n",
            "Collecting opentelemetry-exporter-otlp-proto-common==1.39.1 (from opentelemetry-exporter-otlp-proto-grpc==1.39.1->opentelemetry-exporter-otlp->arize-phoenix->-r requirements.txt (line 1))\n",
            "  Downloading opentelemetry_exporter_otlp_proto_common-1.39.1-py3-none-any.whl.metadata (1.8 kB)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /home/doran/jupyterlab/.venv/lib/python3.12/site-packages (from scikit-learn->arize-phoenix->-r requirements.txt (line 1)) (1.5.3)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /home/doran/jupyterlab/.venv/lib/python3.12/site-packages (from scikit-learn->arize-phoenix->-r requirements.txt (line 1)) (3.6.0)\n",
            "Requirement already satisfied: click>=7.0 in /home/doran/jupyterlab/.venv/lib/python3.12/site-packages (from uvicorn->arize-phoenix->-r requirements.txt (line 1)) (8.3.1)\n",
            "Requirement already satisfied: webencodings in /home/doran/jupyterlab/.venv/lib/python3.12/site-packages (from bleach!=5.0.0->bleach[css]!=5.0.0->nbconvert->jupyter->-r requirements.txt (line 9)) (0.5.1)\n",
            "Requirement already satisfied: tinycss2<1.5,>=1.1.0 in /home/doran/jupyterlab/.venv/lib/python3.12/site-packages (from bleach[css]!=5.0.0->nbconvert->jupyter->-r requirements.txt (line 9)) (1.4.0)\n",
            "Requirement already satisfied: zipp>=3.20 in /home/doran/jupyterlab/.venv/lib/python3.12/site-packages (from importlib-metadata<8.8.0,>=6.0->opentelemetry-api->openinference-instrumentation-langchain->-r requirements.txt (line 3)) (3.23.0)\n",
            "Requirement already satisfied: decorator>=4.3.2 in /home/doran/jupyterlab/.venv/lib/python3.12/site-packages (from ipython>=7.23.1->ipykernel->jupyter->-r requirements.txt (line 9)) (5.2.1)\n",
            "Requirement already satisfied: ipython-pygments-lexers>=1.0.0 in /home/doran/jupyterlab/.venv/lib/python3.12/site-packages (from ipython>=7.23.1->ipykernel->jupyter->-r requirements.txt (line 9)) (1.1.1)\n",
            "Requirement already satisfied: jedi>=0.18.1 in /home/doran/jupyterlab/.venv/lib/python3.12/site-packages (from ipython>=7.23.1->ipykernel->jupyter->-r requirements.txt (line 9)) (0.19.2)\n",
            "Requirement already satisfied: pexpect>4.3 in /home/doran/jupyterlab/.venv/lib/python3.12/site-packages (from ipython>=7.23.1->ipykernel->jupyter->-r requirements.txt (line 9)) (4.9.0)\n",
            "Requirement already satisfied: stack_data>=0.6.0 in /home/doran/jupyterlab/.venv/lib/python3.12/site-packages (from ipython>=7.23.1->ipykernel->jupyter->-r requirements.txt (line 9)) (0.6.3)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /home/doran/jupyterlab/.venv/lib/python3.12/site-packages (from jsonpatch<2.0.0,>=1.33.0->langchain-core<2.0.0,>=1.2.9->langchain->-r requirements.txt (line 4)) (3.0.0)\n",
            "Requirement already satisfied: platformdirs>=2.5 in /home/doran/jupyterlab/.venv/lib/python3.12/site-packages (from jupyter-core!=5.0.*,>=4.12->ipykernel->jupyter->-r requirements.txt (line 9)) (4.5.1)\n",
            "Requirement already satisfied: argon2-cffi>=21.1 in /home/doran/jupyterlab/.venv/lib/python3.12/site-packages (from jupyter-server<3,>=2.4.0->jupyterlab->jupyter->-r requirements.txt (line 9)) (25.1.0)\n",
            "Requirement already satisfied: jupyter-events>=0.11.0 in /home/doran/jupyterlab/.venv/lib/python3.12/site-packages (from jupyter-server<3,>=2.4.0->jupyterlab->jupyter->-r requirements.txt (line 9)) (0.12.0)\n",
            "Requirement already satisfied: jupyter-server-terminals>=0.4.4 in /home/doran/jupyterlab/.venv/lib/python3.12/site-packages (from jupyter-server<3,>=2.4.0->jupyterlab->jupyter->-r requirements.txt (line 9)) (0.5.4)\n",
            "Requirement already satisfied: send2trash>=1.8.2 in /home/doran/jupyterlab/.venv/lib/python3.12/site-packages (from jupyter-server<3,>=2.4.0->jupyterlab->jupyter->-r requirements.txt (line 9)) (2.1.0)\n",
            "Requirement already satisfied: terminado>=0.8.3 in /home/doran/jupyterlab/.venv/lib/python3.12/site-packages (from jupyter-server<3,>=2.4.0->jupyterlab->jupyter->-r requirements.txt (line 9)) (0.18.1)\n",
            "Requirement already satisfied: websocket-client>=1.7 in /home/doran/jupyterlab/.venv/lib/python3.12/site-packages (from jupyter-server<3,>=2.4.0->jupyterlab->jupyter->-r requirements.txt (line 9)) (1.9.0)\n",
            "Requirement already satisfied: babel>=2.10 in /home/doran/jupyterlab/.venv/lib/python3.12/site-packages (from jupyterlab-server<3,>=2.27.1->jupyterlab->jupyter->-r requirements.txt (line 9)) (2.17.0)\n",
            "Requirement already satisfied: json5>=0.9.0 in /home/doran/jupyterlab/.venv/lib/python3.12/site-packages (from jupyterlab-server<3,>=2.27.1->jupyterlab->jupyter->-r requirements.txt (line 9)) (0.13.0)\n",
            "Requirement already satisfied: jsonschema>=4.18.0 in /home/doran/jupyterlab/.venv/lib/python3.12/site-packages (from jupyterlab-server<3,>=2.27.1->jupyterlab->jupyter->-r requirements.txt (line 9)) (4.26.0)\n",
            "Collecting ormsgpack>=1.12.0 (from langgraph-checkpoint<5.0.0,>=2.1.0->langgraph<1.1.0,>=1.0.7->langchain->-r requirements.txt (line 4))\n",
            "  Downloading ormsgpack-1.12.2-cp312-cp312-manylinux_2_17_aarch64.manylinux2014_aarch64.whl.metadata (3.2 kB)\n",
            "Collecting requests-toolbelt>=1.0.0 (from langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.2.9->langchain->-r requirements.txt (line 4))\n",
            "  Downloading requests_toolbelt-1.0.0-py2.py3-none-any.whl.metadata (14 kB)\n",
            "Collecting zstandard>=0.23.0 (from langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.2.9->langchain->-r requirements.txt (line 4))\n",
            "  Downloading zstandard-0.25.0-cp312-cp312-manylinux2014_aarch64.manylinux_2_17_aarch64.whl.metadata (3.3 kB)\n",
            "Collecting cross-web>=0.3.0 (from lia-web>=0.2.1->strawberry-graphql==0.287.3->arize-phoenix->-r requirements.txt (line 1))\n",
            "  Downloading cross_web-0.4.1-py3-none-any.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: fastjsonschema>=2.15 in /home/doran/jupyterlab/.venv/lib/python3.12/site-packages (from nbformat>=5.7->nbconvert->jupyter->-r requirements.txt (line 9)) (2.21.2)\n",
            "Requirement already satisfied: wcwidth in /home/doran/jupyterlab/.venv/lib/python3.12/site-packages (from prompt-toolkit>=3.0.30->jupyter-console->jupyter->-r requirements.txt (line 9)) (0.2.14)\n",
            "Requirement already satisfied: soupsieve>=1.6.1 in /home/doran/jupyterlab/.venv/lib/python3.12/site-packages (from beautifulsoup4->nbconvert->jupyter->-r requirements.txt (line 9)) (2.8.1)\n",
            "Requirement already satisfied: cffi>=2.0.0 in /home/doran/jupyterlab/.venv/lib/python3.12/site-packages (from cryptography->authlib->arize-phoenix->-r requirements.txt (line 1)) (2.0.0)\n",
            "Collecting ply (from jsonpath-ng->arize-phoenix-evals>=2.8.0->arize-phoenix->-r requirements.txt (line 1))\n",
            "  Downloading ply-3.11-py2.py3-none-any.whl.metadata (844 bytes)\n",
            "Requirement already satisfied: argon2-cffi-bindings in /home/doran/jupyterlab/.venv/lib/python3.12/site-packages (from argon2-cffi>=21.1->jupyter-server<3,>=2.4.0->jupyterlab->jupyter->-r requirements.txt (line 9)) (25.1.0)\n",
            "Requirement already satisfied: pycparser in /home/doran/jupyterlab/.venv/lib/python3.12/site-packages (from cffi>=2.0.0->cryptography->authlib->arize-phoenix->-r requirements.txt (line 1)) (2.23)\n",
            "Requirement already satisfied: parso<0.9.0,>=0.8.4 in /home/doran/jupyterlab/.venv/lib/python3.12/site-packages (from jedi>=0.18.1->ipython>=7.23.1->ipykernel->jupyter->-r requirements.txt (line 9)) (0.8.5)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /home/doran/jupyterlab/.venv/lib/python3.12/site-packages (from jsonschema>=4.18.0->jupyterlab-server<3,>=2.27.1->jupyterlab->jupyter->-r requirements.txt (line 9)) (2025.9.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /home/doran/jupyterlab/.venv/lib/python3.12/site-packages (from jsonschema>=4.18.0->jupyterlab-server<3,>=2.27.1->jupyterlab->jupyter->-r requirements.txt (line 9)) (0.37.0)\n",
            "Requirement already satisfied: rpds-py>=0.25.0 in /home/doran/jupyterlab/.venv/lib/python3.12/site-packages (from jsonschema>=4.18.0->jupyterlab-server<3,>=2.27.1->jupyterlab->jupyter->-r requirements.txt (line 9)) (0.30.0)\n",
            "Requirement already satisfied: python-json-logger>=2.0.4 in /home/doran/jupyterlab/.venv/lib/python3.12/site-packages (from jupyter-events>=0.11.0->jupyter-server<3,>=2.4.0->jupyterlab->jupyter->-r requirements.txt (line 9)) (4.0.0)\n",
            "Requirement already satisfied: rfc3339-validator in /home/doran/jupyterlab/.venv/lib/python3.12/site-packages (from jupyter-events>=0.11.0->jupyter-server<3,>=2.4.0->jupyterlab->jupyter->-r requirements.txt (line 9)) (0.1.4)\n",
            "Requirement already satisfied: rfc3986-validator>=0.1.1 in /home/doran/jupyterlab/.venv/lib/python3.12/site-packages (from jupyter-events>=0.11.0->jupyter-server<3,>=2.4.0->jupyterlab->jupyter->-r requirements.txt (line 9)) (0.1.1)\n",
            "Requirement already satisfied: ptyprocess>=0.5 in /home/doran/jupyterlab/.venv/lib/python3.12/site-packages (from pexpect>4.3->ipython>=7.23.1->ipykernel->jupyter->-r requirements.txt (line 9)) (0.7.0)\n",
            "Requirement already satisfied: executing>=1.2.0 in /home/doran/jupyterlab/.venv/lib/python3.12/site-packages (from stack_data>=0.6.0->ipython>=7.23.1->ipykernel->jupyter->-r requirements.txt (line 9)) (2.2.1)\n",
            "Requirement already satisfied: asttokens>=2.1.0 in /home/doran/jupyterlab/.venv/lib/python3.12/site-packages (from stack_data>=0.6.0->ipython>=7.23.1->ipykernel->jupyter->-r requirements.txt (line 9)) (3.0.1)\n",
            "Requirement already satisfied: pure-eval in /home/doran/jupyterlab/.venv/lib/python3.12/site-packages (from stack_data>=0.6.0->ipython>=7.23.1->ipykernel->jupyter->-r requirements.txt (line 9)) (0.2.3)\n",
            "Requirement already satisfied: fqdn in /home/doran/jupyterlab/.venv/lib/python3.12/site-packages (from jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.11.0->jupyter-server<3,>=2.4.0->jupyterlab->jupyter->-r requirements.txt (line 9)) (1.5.1)\n",
            "Requirement already satisfied: isoduration in /home/doran/jupyterlab/.venv/lib/python3.12/site-packages (from jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.11.0->jupyter-server<3,>=2.4.0->jupyterlab->jupyter->-r requirements.txt (line 9)) (20.11.0)\n",
            "Requirement already satisfied: rfc3987-syntax>=1.1.0 in /home/doran/jupyterlab/.venv/lib/python3.12/site-packages (from jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.11.0->jupyter-server<3,>=2.4.0->jupyterlab->jupyter->-r requirements.txt (line 9)) (1.1.0)\n",
            "Requirement already satisfied: uri-template in /home/doran/jupyterlab/.venv/lib/python3.12/site-packages (from jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.11.0->jupyter-server<3,>=2.4.0->jupyterlab->jupyter->-r requirements.txt (line 9)) (1.3.0)\n",
            "Requirement already satisfied: webcolors>=24.6.0 in /home/doran/jupyterlab/.venv/lib/python3.12/site-packages (from jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.11.0->jupyter-server<3,>=2.4.0->jupyterlab->jupyter->-r requirements.txt (line 9)) (25.10.0)\n",
            "Requirement already satisfied: lark>=1.2.2 in /home/doran/jupyterlab/.venv/lib/python3.12/site-packages (from rfc3987-syntax>=1.1.0->jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.11.0->jupyter-server<3,>=2.4.0->jupyterlab->jupyter->-r requirements.txt (line 9)) (1.3.1)\n",
            "Requirement already satisfied: arrow>=0.15.0 in /home/doran/jupyterlab/.venv/lib/python3.12/site-packages (from isoduration->jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.11.0->jupyter-server<3,>=2.4.0->jupyterlab->jupyter->-r requirements.txt (line 9)) (1.4.0)\n",
            "Downloading arize_phoenix-12.33.1-py3-none-any.whl (2.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.6/2.6 MB\u001b[0m \u001b[31m18.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
            "\u001b[?25hDownloading strawberry_graphql-0.287.3-py3-none-any.whl (309 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m309.2/309.2 kB\u001b[0m \u001b[31m29.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading arize_phoenix_otel-0.14.0-py3-none-any.whl (17 kB)\n",
            "Downloading openinference_instrumentation_langchain-0.1.58-py3-none-any.whl (24 kB)\n",
            "Downloading langchain-1.2.9-py3-none-any.whl (111 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m111.2/111.2 kB\u001b[0m \u001b[31m12.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading langchain_openai-1.1.7-py3-none-any.whl (84 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.8/84.8 kB\u001b[0m \u001b[31m8.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading langchain_tavily-0.2.17-py3-none-any.whl (30 kB)\n",
            "Downloading jupyter-1.1.1-py2.py3-none-any.whl (2.7 kB)\n",
            "Downloading alembic-1.18.3-py3-none-any.whl (262 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m262.3/262.3 kB\u001b[0m \u001b[31m23.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading arize_phoenix_client-1.28.0-py3-none-any.whl (149 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m149.5/149.5 kB\u001b[0m \u001b[31m13.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading arize_phoenix_evals-2.9.0-py3-none-any.whl (169 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m169.7/169.7 kB\u001b[0m \u001b[31m17.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading langchain_core-1.2.9-py3-none-any.whl (496 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m496.3/496.3 kB\u001b[0m \u001b[31m33.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading langgraph-1.0.8-py3-none-any.whl (158 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m158.1/158.1 kB\u001b[0m \u001b[31m16.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading openinference_instrumentation-0.1.44-py3-none-any.whl (30 kB)\n",
            "Downloading openinference_semantic_conventions-0.1.26-py3-none-any.whl (10 kB)\n",
            "Downloading opentelemetry_proto-1.39.1-py3-none-any.whl (72 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m72.5/72.5 kB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading sqlalchemy-2.0.46-cp312-cp312-manylinux2014_aarch64.manylinux_2_17_aarch64.manylinux_2_28_aarch64.whl (3.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m70.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
            "\u001b[?25hDownloading sqlean_py-3.50.4.5-cp312-cp312-manylinux_2_28_aarch64.whl (3.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m24.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0mm\n",
            "\u001b[?25hDownloading wrapt-1.17.3-cp312-cp312-manylinux2014_aarch64.manylinux_2_17_aarch64.manylinux_2_28_aarch64.whl (88 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m88.2/88.2 kB\u001b[0m \u001b[31m9.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading aioitertools-0.13.0-py3-none-any.whl (24 kB)\n",
            "Downloading aiosqlite-0.22.1-py3-none-any.whl (17 kB)\n",
            "Downloading authlib-1.6.7-py2.py3-none-any.whl (244 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m244.1/244.1 kB\u001b[0m \u001b[31m24.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading cachetools-7.0.0-py3-none-any.whl (13 kB)\n",
            "Downloading email_validator-2.3.0-py3-none-any.whl (35 kB)\n",
            "Downloading grpc_interceptor-0.15.4-py3-none-any.whl (20 kB)\n",
            "Downloading jmespath-1.1.0-py3-none-any.whl (20 kB)\n",
            "Downloading jupyter_console-6.6.3-py3-none-any.whl (24 kB)\n",
            "Downloading ldap3-2.9.1-py2.py3-none-any.whl (432 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m432.2/432.2 kB\u001b[0m \u001b[31m35.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading notebook-7.5.3-py3-none-any.whl (14.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.5/14.5 MB\u001b[0m \u001b[31m86.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
            "\u001b[?25hDownloading jupyterlab-4.5.3-py3-none-any.whl (12.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.4/12.4 MB\u001b[0m \u001b[31m86.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
            "\u001b[?25hDownloading opentelemetry_api-1.39.1-py3-none-any.whl (66 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m66.4/66.4 kB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading opentelemetry_exporter_otlp-1.39.1-py3-none-any.whl (7.0 kB)\n",
            "Downloading opentelemetry_exporter_otlp_proto_grpc-1.39.1-py3-none-any.whl (19 kB)\n",
            "Downloading opentelemetry_exporter_otlp_proto_http-1.39.1-py3-none-any.whl (19 kB)\n",
            "Downloading opentelemetry_exporter_otlp_proto_common-1.39.1-py3-none-any.whl (18 kB)\n",
            "Downloading opentelemetry_sdk-1.39.1-py3-none-any.whl (132 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m132.6/132.6 kB\u001b[0m \u001b[31m14.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading opentelemetry_semantic_conventions-0.60b1-py3-none-any.whl (219 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m220.0/220.0 kB\u001b[0m \u001b[31m19.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading opentelemetry_instrumentation-0.60b1-py3-none-any.whl (33 kB)\n",
            "Downloading dnspython-2.8.0-py3-none-any.whl (331 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m331.1/331.1 kB\u001b[0m \u001b[31m29.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading graphql_core-3.2.7-py3-none-any.whl (207 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.3/207.3 kB\u001b[0m \u001b[31m16.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading greenlet-3.3.1-cp312-cp312-manylinux_2_24_aarch64.manylinux_2_28_aarch64.whl (597 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m597.4/597.4 kB\u001b[0m \u001b[31m40.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading jsonpatch-1.33-py2.py3-none-any.whl (12 kB)\n",
            "Downloading langgraph_checkpoint-4.0.0-py3-none-any.whl (46 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.3/46.3 kB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading langgraph_prebuilt-1.0.7-py3-none-any.whl (35 kB)\n",
            "Downloading langgraph_sdk-0.3.4-py3-none-any.whl (67 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.8/67.8 kB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading langsmith-0.6.9-py3-none-any.whl (319 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m319.2/319.2 kB\u001b[0m \u001b[31m28.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading lia_web-0.3.1-py3-none-any.whl (5.9 kB)\n",
            "Downloading uuid_utils-0.14.0-cp39-abi3-manylinux_2_17_aarch64.manylinux2014_aarch64.whl (340 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m340.5/340.5 kB\u001b[0m \u001b[31m30.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading jsonpath_ng-1.7.0-py3-none-any.whl (30 kB)\n",
            "Downloading mako-1.3.10-py3-none-any.whl (78 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.5/78.5 kB\u001b[0m \u001b[31m8.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pystache-0.6.8-py3-none-any.whl (82 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m82.1/82.1 kB\u001b[0m \u001b[31m9.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading cross_web-0.4.1-py3-none-any.whl (14 kB)\n",
            "Downloading ormsgpack-1.12.2-cp312-cp312-manylinux_2_17_aarch64.manylinux2014_aarch64.whl (203 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m203.2/203.2 kB\u001b[0m \u001b[31m20.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading requests_toolbelt-1.0.0-py2.py3-none-any.whl (54 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.5/54.5 kB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading zstandard-0.25.0-cp312-cp312-manylinux2014_aarch64.manylinux_2_17_aarch64.whl (5.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.1/5.1 MB\u001b[0m \u001b[31m14.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0mm\n",
            "\u001b[?25hDownloading ply-3.11-py2.py3-none-any.whl (49 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.6/49.6 kB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: ply, zstandard, wrapt, uuid-utils, sqlean-py, pystache, ormsgpack, opentelemetry-proto, openinference-semantic-conventions, Mako, ldap3, jsonpath-ng, jsonpatch, jmespath, greenlet, graphql-core, dnspython, cross-web, cachetools, aiosqlite, aioitertools, sqlalchemy, requests-toolbelt, opentelemetry-exporter-otlp-proto-common, opentelemetry-api, lia-web, grpc-interceptor, email-validator, strawberry-graphql, opentelemetry-semantic-conventions, langsmith, langgraph-sdk, authlib, alembic, opentelemetry-sdk, opentelemetry-instrumentation, langchain-core, jupyter-console, opentelemetry-exporter-otlp-proto-http, opentelemetry-exporter-otlp-proto-grpc, openinference-instrumentation, langgraph-checkpoint, langchain-openai, opentelemetry-exporter-otlp, openinference-instrumentation-langchain, langgraph-prebuilt, arize-phoenix-evals, langgraph, arize-phoenix-otel, arize-phoenix-client, langchain, arize-phoenix, langchain-tavily, jupyterlab, notebook, jupyter\n",
            "  Attempting uninstall: wrapt\n",
            "    Found existing installation: wrapt 2.0.1\n",
            "    Uninstalling wrapt-2.0.1:\n",
            "      Successfully uninstalled wrapt-2.0.1\n",
            "  Attempting uninstall: jupyterlab\n",
            "    Found existing installation: jupyterlab 4.4.7\n",
            "    Uninstalling jupyterlab-4.4.7:\n",
            "      Successfully uninstalled jupyterlab-4.4.7\n",
            "Successfully installed Mako-1.3.10 aioitertools-0.13.0 aiosqlite-0.22.1 alembic-1.18.3 arize-phoenix-12.33.1 arize-phoenix-client-1.28.0 arize-phoenix-evals-2.9.0 arize-phoenix-otel-0.14.0 authlib-1.6.7 cachetools-7.0.0 cross-web-0.4.1 dnspython-2.8.0 email-validator-2.3.0 graphql-core-3.2.7 greenlet-3.3.1 grpc-interceptor-0.15.4 jmespath-1.1.0 jsonpatch-1.33 jsonpath-ng-1.7.0 jupyter-1.1.1 jupyter-console-6.6.3 jupyterlab-4.5.3 langchain-1.2.9 langchain-core-1.2.9 langchain-openai-1.1.7 langchain-tavily-0.2.17 langgraph-1.0.8 langgraph-checkpoint-4.0.0 langgraph-prebuilt-1.0.7 langgraph-sdk-0.3.4 langsmith-0.6.9 ldap3-2.9.1 lia-web-0.3.1 notebook-7.5.3 openinference-instrumentation-0.1.44 openinference-instrumentation-langchain-0.1.58 openinference-semantic-conventions-0.1.26 opentelemetry-api-1.39.1 opentelemetry-exporter-otlp-1.39.1 opentelemetry-exporter-otlp-proto-common-1.39.1 opentelemetry-exporter-otlp-proto-grpc-1.39.1 opentelemetry-exporter-otlp-proto-http-1.39.1 opentelemetry-instrumentation-0.60b1 opentelemetry-proto-1.39.1 opentelemetry-sdk-1.39.1 opentelemetry-semantic-conventions-0.60b1 ormsgpack-1.12.2 ply-3.11 pystache-0.6.8 requests-toolbelt-1.0.0 sqlalchemy-2.0.46 sqlean-py-3.50.4.5 strawberry-graphql-0.287.3 uuid-utils-0.14.0 wrapt-1.17.3 zstandard-0.25.0\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ],
      "source": [
        "# First step: install dependencies\n",
        "from pathlib import Path\n",
        "\n",
        "candidates = [\n",
        "    Path(\"requirements.txt\"),\n",
        "    Path(\"article-scripts/arize/requirements.txt\"),\n",
        "]\n",
        "requirements_file = next((p for p in candidates if p.exists()), None)\n",
        "if requirements_file is None:\n",
        "    raise FileNotFoundError(\"Could not find requirements.txt for this notebook\")\n",
        "\n",
        "print(f\"Installing dependencies from {requirements_file.resolve()}\")\n",
        "%pip install -r {requirements_file}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "2e67bfe7",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "12.33.1\n"
          ]
        }
      ],
      "source": [
        "import phoenix\n",
        "print(phoenix.__version__)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "383d6c4b",
      "metadata": {},
      "source": [
        "# LangChain Agent Monitoring with Arize Phoenix\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "910f2bc1",
      "metadata": {},
      "outputs": [],
      "source": [
        "\"\"\"Monitor a local LangChain agent with Arize Phoenix and a vLLM backend.\n",
        "\n",
        "This script is notebook-friendly and can also be run as a CLI tool.\n",
        "\"\"\"\n",
        "\n",
        "from __future__ import annotations\n",
        "\n",
        "import argparse\n",
        "import os\n",
        "import sys\n",
        "from typing import Any\n",
        "from urllib.error import URLError\n",
        "from urllib.parse import urlsplit, urlunsplit\n",
        "from urllib.request import urlopen\n",
        "\n",
        "try:\n",
        "    from dotenv import load_dotenv\n",
        "except ImportError:  # pragma: no cover - optional at runtime\n",
        "    load_dotenv = None\n",
        "\n",
        "\n",
        "def _load_env() -> None:\n",
        "    if load_dotenv is not None:\n",
        "        load_dotenv()\n",
        "\n",
        "\n",
        "def _require_env(name: str) -> str:\n",
        "    value = os.getenv(name)\n",
        "    if not value:\n",
        "        raise RuntimeError(f\"Missing required environment variable: {name}\")\n",
        "    return value\n",
        "\n",
        "\n",
        "def _validate_collector_endpoint(endpoint: str) -> None:\n",
        "    if not endpoint.startswith((\"http://\", \"https://\")):\n",
        "        return\n",
        "\n",
        "    parts = urlsplit(endpoint)\n",
        "    base_url = urlunsplit((parts.scheme, parts.netloc, \"\", \"\", \"\"))\n",
        "    healthz_url = base_url.rstrip(\"/\") + \"/healthz\"\n",
        "    try:\n",
        "        with urlopen(healthz_url, timeout=3) as response:\n",
        "            if response.status >= 400:\n",
        "                raise RuntimeError(f\"Phoenix health check failed with HTTP {response.status}\")\n",
        "    except (URLError, TimeoutError, RuntimeError) as exc:\n",
        "        raise RuntimeError(\n",
        "            f\"Phoenix collector is not reachable at {endpoint}. \"\n",
        "            \"Set PHOENIX_COLLECTOR_ENDPOINT to your Phoenix URL (for this cluster: \"\n",
        "            \"http://192.168.86.208:6006) or run a port-forward to localhost:6006.\"\n",
        "        ) from exc\n",
        "\n",
        "\n",
        "def _normalize_collector_endpoint(endpoint: str, protocol: str | None) -> str:\n",
        "    if protocol != \"http/protobuf\":\n",
        "        return endpoint\n",
        "    if not endpoint.startswith((\"http://\", \"https://\")):\n",
        "        return endpoint\n",
        "\n",
        "    parts = urlsplit(endpoint)\n",
        "    if parts.path not in (\"\", \"/\"):\n",
        "        return endpoint\n",
        "\n",
        "    return urlunsplit((parts.scheme, parts.netloc, \"/v1/traces\", parts.query, parts.fragment))\n",
        "\n",
        "\n",
        "def setup_phoenix(project_name: str) -> None:\n",
        "    \"\"\"Enable Phoenix tracing for LangChain runs.\"\"\"\n",
        "    os.environ.setdefault(\"PHOENIX_HOST\", os.getenv(\"PHOENIX_HOST\", \"localhost\"))\n",
        "    os.environ.setdefault(\"PHOENIX_PORT\", os.getenv(\"PHOENIX_PORT\", \"6006\"))\n",
        "\n",
        "    endpoint = os.getenv(\"PHOENIX_COLLECTOR_ENDPOINT\")\n",
        "    if not endpoint:\n",
        "        endpoint = f\"http://{os.environ['PHOENIX_HOST']}:{os.environ['PHOENIX_PORT']}\"\n",
        "    protocol = os.getenv(\"PHOENIX_COLLECTOR_PROTOCOL\")\n",
        "    if not protocol and endpoint.startswith((\"http://\", \"https://\")):\n",
        "        protocol = \"http/protobuf\"\n",
        "    endpoint = _normalize_collector_endpoint(endpoint, protocol)\n",
        "    _validate_collector_endpoint(endpoint)\n",
        "\n",
        "    errors: list[str] = []\n",
        "\n",
        "    # Modern path (Arize Phoenix + OpenInference packages).\n",
        "    try:\n",
        "        from phoenix.otel import register  # type: ignore\n",
        "        from openinference.instrumentation.langchain import (\n",
        "            LangChainInstrumentor,  # type: ignore\n",
        "        )\n",
        "\n",
        "        tracer_provider = register(\n",
        "            project_name=project_name,\n",
        "            endpoint=endpoint,\n",
        "            protocol=protocol,\n",
        "            batch=True,\n",
        "            set_global_tracer_provider=False,\n",
        "        )\n",
        "        LangChainInstrumentor().instrument(tracer_provider=tracer_provider)\n",
        "        return\n",
        "    except SyntaxError as exc:  # pragma: no cover - clearer wrong-package hint\n",
        "        raise RuntimeError(\n",
        "            \"Detected a non-Arize `phoenix` package in this environment. \"\n",
        "            \"Run: `pip uninstall -y phoenix && pip install -U arize-phoenix arize-phoenix-otel`\"\n",
        "        ) from exc\n",
        "    except Exception as exc:  # pragma: no cover - runtime compatibility path\n",
        "        errors.append(f\"modern openinference path failed: {exc}\")\n",
        "\n",
        "    # Legacy path kept for older Phoenix installs.\n",
        "    try:\n",
        "        from phoenix.trace import LangChainInstrumentor  # type: ignore\n",
        "\n",
        "        LangChainInstrumentor().instrument(project_name=project_name)\n",
        "        return\n",
        "    except SyntaxError as exc:  # pragma: no cover - clearer wrong-package hint\n",
        "        raise RuntimeError(\n",
        "            \"Detected a non-Arize `phoenix` package in this environment. \"\n",
        "            \"Run: `pip uninstall -y phoenix && pip install -U arize-phoenix arize-phoenix-otel`\"\n",
        "        ) from exc\n",
        "    except Exception as exc:  # pragma: no cover - runtime compatibility path\n",
        "        errors.append(f\"legacy phoenix.trace path failed: {exc}\")\n",
        "\n",
        "    raise RuntimeError(\n",
        "        \"Unable to configure Phoenix instrumentation. \"\n",
        "        + \" | \".join(errors)\n",
        "        + \" | verify packages: arize-phoenix, arize-phoenix-otel, \"\n",
        "          \"openinference-instrumentation-langchain\"\n",
        "    )\n",
        "\n",
        "\n",
        "def build_llm() -> Any:\n",
        "    \"\"\"Create a ChatOpenAI-compatible client against local vLLM.\"\"\"\n",
        "    model_name = _require_env(\"VLLM_MODEL_NAME\")\n",
        "    base_url = _require_env(\"VLLM_API_BASE\")\n",
        "    api_key = os.getenv(\"OPENAI_API_KEY\", \"not-needed\")\n",
        "    temperature = float(os.getenv(\"LLM_TEMPERATURE\", \"0\"))\n",
        "\n",
        "    # Preferred package (current LangChain split).\n",
        "    try:\n",
        "        from langchain_openai import ChatOpenAI  # type: ignore\n",
        "\n",
        "        return ChatOpenAI(\n",
        "            model=model_name,\n",
        "            base_url=base_url,\n",
        "            api_key=api_key,\n",
        "            temperature=temperature,\n",
        "        )\n",
        "    except Exception:\n",
        "        pass\n",
        "\n",
        "    # Backward-compatible fallback.\n",
        "    from langchain.chat_models import ChatOpenAI  # type: ignore\n",
        "\n",
        "    return ChatOpenAI(\n",
        "        model_name=model_name,\n",
        "        openai_api_base=base_url,\n",
        "        openai_api_key=api_key,\n",
        "        temperature=temperature,\n",
        "    )\n",
        "\n",
        "\n",
        "def build_search_tool() -> Any:\n",
        "    \"\"\"Create Tavily tool used by the agent.\"\"\"\n",
        "    _require_env(\"TAVILY_API_KEY\")\n",
        "\n",
        "    max_results = int(os.getenv(\"TAVILY_MAX_RESULTS\", \"5\"))\n",
        "    topic = os.getenv(\"TAVILY_TOPIC\", \"general\")\n",
        "\n",
        "    try:\n",
        "        from langchain_tavily import TavilySearch  # type: ignore\n",
        "\n",
        "        return TavilySearch(max_results=max_results, topic=topic)\n",
        "    except Exception:\n",
        "        # Fallback if using langchain-community integration.\n",
        "        from langchain_community.tools.tavily_search import TavilySearchResults  # type: ignore\n",
        "\n",
        "        return TavilySearchResults(max_results=max_results)\n",
        "\n",
        "\n",
        "def _extract_message_content(message: Any) -> str:\n",
        "    \"\"\"Normalize LangChain message-like outputs to plain text.\"\"\"\n",
        "    if isinstance(message, str):\n",
        "        return message\n",
        "\n",
        "    content = getattr(message, \"content\", None)\n",
        "    if isinstance(content, str):\n",
        "        return content\n",
        "    if isinstance(content, list):\n",
        "        parts: list[str] = []\n",
        "        for part in content:\n",
        "            if isinstance(part, str):\n",
        "                parts.append(part)\n",
        "            elif isinstance(part, dict) and \"text\" in part:\n",
        "                parts.append(str(part[\"text\"]))\n",
        "        if parts:\n",
        "            return \"\\n\".join(parts)\n",
        "\n",
        "    if isinstance(message, dict):\n",
        "        if \"output\" in message:\n",
        "            return str(message[\"output\"])\n",
        "        if \"content\" in message:\n",
        "            return str(message[\"content\"])\n",
        "\n",
        "    return str(message)\n",
        "\n",
        "\n",
        "def run_agent(question: str) -> str:\n",
        "    llm = build_llm()\n",
        "    search_tool = build_search_tool()\n",
        "\n",
        "    # LangChain >=1.0 API.\n",
        "    try:\n",
        "        from langchain.agents import create_agent\n",
        "\n",
        "        agent = create_agent(\n",
        "            model=llm,\n",
        "            tools=[search_tool],\n",
        "            system_prompt=(\n",
        "                \"You are a concise assistant. Use web search when needed and cite key facts.\"\n",
        "            ),\n",
        "            debug=True,\n",
        "        )\n",
        "        result = agent.invoke({\"messages\": [{\"role\": \"user\", \"content\": question}]})\n",
        "        if isinstance(result, dict) and \"messages\" in result and result[\"messages\"]:\n",
        "            return _extract_message_content(result[\"messages\"][-1])\n",
        "        return _extract_message_content(result)\n",
        "    except Exception:\n",
        "        pass\n",
        "\n",
        "    # LangChain <1.0 API fallback.\n",
        "    try:\n",
        "        from langchain.agents import AgentType, initialize_agent\n",
        "\n",
        "        agent = initialize_agent(\n",
        "            tools=[search_tool],\n",
        "            llm=llm,\n",
        "            agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION,\n",
        "            verbose=True,\n",
        "        )\n",
        "\n",
        "        if hasattr(agent, \"invoke\"):\n",
        "            result = agent.invoke({\"input\": question})\n",
        "            return _extract_message_content(result)\n",
        "        if hasattr(agent, \"run\"):\n",
        "            return _extract_message_content(agent.run(question))\n",
        "    except Exception:\n",
        "        pass\n",
        "\n",
        "    raise RuntimeError(\"Unsupported LangChain agent interface\")\n",
        "\n",
        "\n",
        "def parse_args() -> argparse.Namespace:\n",
        "    parser = argparse.ArgumentParser(description=__doc__)\n",
        "    parser.add_argument(\n",
        "        \"--project\",\n",
        "        default=os.getenv(\"PHOENIX_PROJECT_NAME\", \"local-llm-agent\"),\n",
        "        help=\"Phoenix project name\",\n",
        "    )\n",
        "    parser.add_argument(\n",
        "        \"--question\",\n",
        "        default=os.getenv(\n",
        "            \"QUESTION\",\n",
        "            \"Who is the current president of the United States and what is the latest headline about them?\",\n",
        "        ),\n",
        "        help=\"Question to run through the agent\",\n",
        "    )\n",
        "    return parser.parse_args()\n",
        "\n",
        "\n",
        "def main() -> int:\n",
        "    _load_env()\n",
        "    args = parse_args()\n",
        "\n",
        "    setup_phoenix(project_name=args.project)\n",
        "    answer = run_agent(args.question)\n",
        "\n",
        "    print(\"\\n=== Agent Response ===\")\n",
        "    print(answer)\n",
        "    print(\"\\nOpen Phoenix UI and inspect project:\", args.project)\n",
        "    print(\n",
        "        \"Collector target:\",\n",
        "        os.getenv(\"PHOENIX_COLLECTOR_ENDPOINT\", f\"http://{os.getenv('PHOENIX_HOST', 'localhost')}:{os.getenv('PHOENIX_PORT', '6006')}\"),\n",
        "    )\n",
        "    return 0\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "d87c176e",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1. Find the latest two major U.S. AI policy headlines and explain how they could affect open-source model deployment.\n",
            "2. Identify three recent LLM failure incidents in production, summarize each root cause, and propose concrete mitigations.\n",
            "3. Compare LangChain and LlamaIndex for a retrieval app: decision matrix, trade-offs, and recommendation for a 2-engineer team.\n",
            "4. Find a cybersecurity incident from the last 48 hours and explain potential risks to self-hosted vLLM infrastructure.\n",
            "5. Design a one-week experiment plan to detect prompt regressions using Phoenix traces, with hypotheses and success metrics.\n",
            "6. Find two conflicting reports about one current event, then separate verified facts from uncertainty.\n",
            "7. What are the top 5 mistakes teams make when instrumenting LLM apps, and how can Phoenix traces reveal each one?\n",
            "\n",
            "Selected question (7): What are the top 5 mistakes teams make when instrumenting LLM apps, and how can Phoenix traces reveal each one?\n"
          ]
        }
      ],
      "source": [
        "# Optional: pick an interesting question for this run\n",
        "interesting_questions = [\n",
        "    \"Find the latest two major U.S. AI policy headlines and explain how they could affect open-source model deployment.\",\n",
        "    \"Identify three recent LLM failure incidents in production, summarize each root cause, and propose concrete mitigations.\",\n",
        "    \"Compare LangChain and LlamaIndex for a retrieval app: decision matrix, trade-offs, and recommendation for a 2-engineer team.\",\n",
        "    \"Find a cybersecurity incident from the last 48 hours and explain potential risks to self-hosted vLLM infrastructure.\",\n",
        "    \"Design a one-week experiment plan to detect prompt regressions using Phoenix traces, with hypotheses and success metrics.\",\n",
        "    \"Find two conflicting reports about one current event, then separate verified facts from uncertainty.\",\n",
        "    \"What are the top 5 mistakes teams make when instrumenting LLM apps, and how can Phoenix traces reveal each one?\",\n",
        "]\n",
        "\n",
        "for i, prompt in enumerate(interesting_questions, start=1):\n",
        "    print(f\"{i}. {prompt}\")\n",
        "\n",
        "selected_index = 7  # Change this to 1..7\n",
        "question = interesting_questions[selected_index - 1]\n",
        "print(f\"\\nSelected question ({selected_index}): {question}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "53499ac6",
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Overriding of current TracerProvider is not allowed\n",
            "Attempting to instrument while already instrumented\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "🔭 OpenTelemetry Tracing Details 🔭\n",
            "|  Phoenix Project: local-llm-agent\n",
            "|  Span Processor: SimpleSpanProcessor\n",
            "|  Collector Endpoint: http://192.168.86.208:6006/v1/traces\n",
            "|  Transport: HTTP + protobuf\n",
            "|  Transport Headers: {}\n",
            "|  \n",
            "|  Using a default SpanProcessor. `add_span_processor` will overwrite this default.\n",
            "|  \n",
            "|  ⚠️ WARNING: It is strongly advised to use a BatchSpanProcessor in production environments.\n",
            "|  \n",
            "|  `register` has set this TracerProvider as the global OpenTelemetry default.\n",
            "|  To disable this behavior, call `register` with `set_global_tracer_provider=False`.\n",
            "\n",
            "\u001b[1m[values]\u001b[0m {'messages': [HumanMessage(content='What are the top 5 mistakes teams make when instrumenting LLM apps, and how can Phoenix traces reveal each one?', additional_kwargs={}, response_metadata={}, id='063bd29e-22b9-483e-8378-16fb93e9f73f')]}\n",
            "\u001b[1m[updates]\u001b[0m {'model': {'messages': [AIMessage(content='', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 65, 'prompt_tokens': 1822, 'total_tokens': 1887, 'completion_tokens_details': None, 'prompt_tokens_details': None}, 'model_provider': 'openai', 'model_name': 'Qwen/Qwen3-Coder-Next-FP8', 'system_fingerprint': None, 'id': 'chatcmpl-ad7f06cd56fcf084', 'finish_reason': 'tool_calls', 'logprobs': None}, id='lc_run--019c399b-59f3-7b00-b89d-cfe552369274-0', tool_calls=[{'name': 'tavily_search', 'args': {'query': 'top 5 mistakes teams make when instrumenting LLM apps'}, 'id': 'chatcmpl-tool-a9cdc035189a5a19', 'type': 'tool_call'}, {'name': 'tavily_search', 'args': {'query': 'how Phoenix traces reveal LLM app instrumentation mistakes'}, 'id': 'chatcmpl-tool-915c8c4fe212767b', 'type': 'tool_call'}], invalid_tool_calls=[], usage_metadata={'input_tokens': 1822, 'output_tokens': 65, 'total_tokens': 1887, 'input_token_details': {}, 'output_token_details': {}})]}}\n",
            "\u001b[1m[values]\u001b[0m {'messages': [HumanMessage(content='What are the top 5 mistakes teams make when instrumenting LLM apps, and how can Phoenix traces reveal each one?', additional_kwargs={}, response_metadata={}, id='063bd29e-22b9-483e-8378-16fb93e9f73f'), AIMessage(content='', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 65, 'prompt_tokens': 1822, 'total_tokens': 1887, 'completion_tokens_details': None, 'prompt_tokens_details': None}, 'model_provider': 'openai', 'model_name': 'Qwen/Qwen3-Coder-Next-FP8', 'system_fingerprint': None, 'id': 'chatcmpl-ad7f06cd56fcf084', 'finish_reason': 'tool_calls', 'logprobs': None}, id='lc_run--019c399b-59f3-7b00-b89d-cfe552369274-0', tool_calls=[{'name': 'tavily_search', 'args': {'query': 'top 5 mistakes teams make when instrumenting LLM apps'}, 'id': 'chatcmpl-tool-a9cdc035189a5a19', 'type': 'tool_call'}, {'name': 'tavily_search', 'args': {'query': 'how Phoenix traces reveal LLM app instrumentation mistakes'}, 'id': 'chatcmpl-tool-915c8c4fe212767b', 'type': 'tool_call'}], invalid_tool_calls=[], usage_metadata={'input_tokens': 1822, 'output_tokens': 65, 'total_tokens': 1887, 'input_token_details': {}, 'output_token_details': {}})]}\n",
            "\u001b[1m[updates]\u001b[0m {'tools': {'messages': [ToolMessage(content='{\"query\": \"how Phoenix traces reveal LLM app instrumentation mistakes\", \"follow_up_questions\": null, \"answer\": null, \"images\": [], \"results\": [{\"url\": \"https://www.linkedin.com/posts/mikeldking_today-arize-ai-phoenix-is-launching-llm-traces-activity-7114625085629501440-5Y0w\", \"title\": \"Today Arize AI Phoenix is launching LLM Traces and Observability ...\", \"content\": \"Today Arize AI Phoenix is launching LLM Traces and Observability to help us understand complex LLM applications and to prompt us to ask questions about these system\\'s inner workings. In order to be able to ask those questions of a system, LLM applications must be properly instrumented and must emit signals in the form of traces. Phoenix now ships with two critical features of LLM Observability: - LLM Traces - observability instrumentation for the two LLM frameworks LlamaIndex and LangChain - LLM Evals - a benchmarked set of LLM-powered evaluations for uncovering hard to identify problems such as hallucinations and retrieval relevancy arize-phoenix is entirely open-source, built on open standards, and runs entirely in the privacy of your python notebook. Let me show you how to troubleshoot an LLM application using Phoenix tracing. You can see that the LLM application is answering questions about documentation provided by arise. Phoenix tracing gives you visibility into the individual steps of your LLM application, helping you identify, troubleshoot, evaluate, and ultimately improving your system.\", \"score\": 0.78886026, \"raw_content\": null}, {\"url\": \"https://www.youtube.com/watch?v=j5WwaknZVDY\", \"title\": \"Understanding Tracing and Instrumentation with Arize Phoenix\", \"content\": \"Understanding Tracing and Instrumentation with Arize Phoenix\\\\nData Science Dojo\\\\n119000 subscribers\\\\n12 likes\\\\n2156 views\\\\n17 May 2025\\\\n🔍 Instrumenting and Tracing LLM Agents with Arize Phoenix | Full Walkthrough & What-If Analysis\\\\n\\\\nIn this video, we take a deep dive into instrumentation, tracing, and evaluation of LLM-powered applications using Arize Phoenix. Learn how spans and traces work behind the scenes, how to manually and automatically instrument your code, and how Phoenix visualizes your agent\\'s logic in real time.\\\\n\\\\n🎯 What You\\'ll Learn:\\\\n\\\\nWhat is instrumentation and why it matters for observability\\\\n\\\\nUnderstanding spans, traces, and how they map to LLM workflows\\\\n\\\\nManual vs. auto-instrumentation techniques\\\\n\\\\nExploring JSON span data inside Phoenix\\\\n\\\\nHow to use What-If Analysis to compare model responses side-by-side\\\\n\\\\nTroubleshooting Phoenix onboarding issues\\\\n\\\\nWhether you\\'re using OpenAI, LangChain, or Anthropic, see how Phoenix makes it easier to debug, evaluate, and optimize your agents—without the guesswork.\\\\n\\\\n💡 Don\\'t forget to like, subscribe, and drop a comment if you want a hands-on demo or walkthrough of a specific part of Phoenix!\\\\n\\\\n\", \"score\": 0.60274047, \"raw_content\": null}, {\"url\": \"https://rajeevbarnwal.medium.com/debugging-and-tracing-llms-like-a-pro-b560ded19fd9\", \"title\": \"Debugging and Tracing LLMs Like a Pro - Rajeev Barnwal\", \"content\": \"As a seasoned AI Leader with over a decade of experience, I have spent years building AI systems where a single user request flows through prompts, chains, tools, retrievers, and external APIs. Traditional methods often fall short, leaving developers sifting through logs or guessing at issues. That is why I am excited to share my insights on **Phoenix** by **Arize AI**, a game changing open source tool that has transformed how I monitor and trace **LLM** pipelines. In this article, I will walk you through what **Phoenix** offers, why it stands out in the 2025 landscape of **LLM** observability tools, and a step by step integration with **LangChain**. **Phoenix** is an open source observability and debugging tool tailored for large language model applications. Step 2: Register Phoenix with OpenTelemetry and Instrument LangChain. ## Wrapping Up. In my experience, Arize Phoenix makes it incredibly easy to debug, trace, and monitor LLM applications.\", \"score\": 0.5809471, \"raw_content\": null}, {\"url\": \"https://arize.com/docs/phoenix/tracing/tutorial\", \"title\": \"Tracing Tutorial - Phoenix - Arize AI\", \"content\": \"##### Tracing. * Chapter 1: Your First Traces. # Tracing Tutorial. Build a fully observable AI agent from scratch - trace every operation, measure quality, and debug conversations. For AI applications, this means capturing every LLM call, tool execution, retrieval operation, and generation - along with their inputs, outputs, latency, and token usage. Phoenix provides the infrastructure for AI observability: **tracing** to capture execution flow, **annotations** to measure quality, and **sessions** to track conversations. As you build each feature, you’ll add the corresponding observability layer - so you can see exactly how classification decisions are made, why certain documents are retrieved, and whether conversations maintain context across turns. # \\u200b Chapter 1: Your First Traces. * Trace LLM calls, tool executions, and RAG retrievals automatically. By the end of this tutorial, you’ll have a working agent with complete observability - the same patterns used by teams running AI applications in production. **Start with Chapter 1: Your First Traces →**.\", \"score\": 0.52232456, \"raw_content\": null}, {\"url\": \"https://mohdzain.com/blogs/tracing-agentic-llm-workflows-arize-phoenix-langgraph\", \"title\": \"Tracing Agentic LLMs with Arize Phoenix & LangGraph - Mohd Zain\", \"content\": \"### Tracing Agentic LLM Workflows Locally Using Arize Phoenix and LangGraph. In this post, I’ll show how I traced a **LangGraph** based agent system locally using **Arize Phoenix**, powered by **OpenTelemetry**, and how I wrapped custom agent spans to get meaningful insights beyond default LLM traces. #### What Is LLM Tracing and Why It Matters for Agentic Systems. Agentic LLM systems behave like distributed systems, making OpenTelemetry a natural fit for observability. #### Running Arize Phoenix Locally for LLM Tracing. Wraps each LangGraph node with a custom OpenTelemetry span, allowing agent-level visibility into execution flow, routing decisions, and token usage without modifying business logic. Demonstrates how the tracing setup and agent decorator are applied directly to LangGraph nodes, ensuring each agent execution is captured as a first-class trace span. Once the agentic workflow runs, navigate to the Phoenix UI at `http://localhost:6006` to visualize the traces. You’ll see a hierarchical trace graph showing agent spans, LLM calls, tool invocations, and custom attributes like token usage and routing decisions.\", \"score\": 0.39480802, \"raw_content\": null}], \"response_time\": 0.82, \"request_id\": \"a5cd669f-54c2-4c06-a148-ce1d9a9e4204\"}', name='tavily_search', id='f90b347d-3d63-402f-9642-375f495f887a', tool_call_id='chatcmpl-tool-915c8c4fe212767b')]}}\n",
            "\u001b[1m[updates]\u001b[0m {'tools': {'messages': [ToolMessage(content='{\"query\": \"top 5 mistakes teams make when instrumenting LLM apps\", \"follow_up_questions\": null, \"answer\": null, \"images\": [], \"results\": [{\"url\": \"https://www.linkedin.com/pulse/8-most-common-mistakes-building-llm-applications-2024-guy-korland-jmz6f\", \"title\": \"The 8 Most-Common Mistakes in Building LLM Applications in 2024 ...\", \"content\": \"Many companies built LLM applications that relied exclusively on the model\\'s pre-trained knowledge, ignoring the fundamental limitations of this approach. I observed numerous teams struggle with fundamental RAG components: poorly designed chunking strategies that fragmented context, embedding models mismatched to domain-specific content, and retrieval systems that couldn\\'t distinguish between semantically similar but contextually irrelevant information. The organizations that truly excelled built RAG systems capable of adapting retrieval strategies based on query types, incorporating human feedback to improve retrieval quality over time, and implementing proper citation mechanisms to maintain provenance and traceability of information. Many developers implemented popular LLM patterns and architectures without evaluating their fit for specific business needs. Successful organizations took time to understand the fundamental capabilities and limitations of different architectural patterns, selecting and adapting approaches based on their specific use cases rather than chasing trending GitHub repositories. The most successful implementations carefully mapped how LLM capabilities would augment existing processes, identifying clear handoff points between AI and human operators, and ensuring seamless data flow between LLM applications and other enterprise systems.\", \"score\": 0.7125728, \"raw_content\": null}, {\"url\": \"https://medium.com/@yadav.navya1601/what-i-learned-watching-teams-debug-llms-in-production-f9f959f574a6\", \"title\": \"What I Learned Watching Teams Debug LLMs in Production - Medium\", \"content\": \"Mistake #3: Black Box Failures. Here\\'s a scenario I see constantly: a user reports “the AI gave me a wrong answer.” The team checks the logs.\", \"score\": 0.68697983, \"raw_content\": null}, {\"url\": \"https://leobit.com/blog/5-common-mistakes-with-llms/\", \"title\": \"5 Common Mistakes with LLMs and Their Impact on Business - Leobit\", \"content\": \"A major strategic mistake companies make is treating AI solely as a technical product to build, rather than a core skill to develop across your\", \"score\": 0.65966886, \"raw_content\": null}, {\"url\": \"https://www.reddit.com/r/AI_Agents/comments/1nax7v1/one_year_as_an_ai_engineer_the_5_biggest/\", \"title\": \"The 5 biggest misconceptions about LLM reliability I\\'ve encountered\", \"content\": \"The bottom line: LLM reliability is a systems engineering problem, not just a model problem. You need proper observability, robust evaluation\", \"score\": 0.6255073, \"raw_content\": null}, {\"url\": \"https://aijourn.com/llm-observability-for-mobile-apps-what-to-instrument-without-violating-privacy/\", \"title\": \"LLM Observability For Mobile Apps: What To Instrument Without ...\", \"content\": \"Unreliable sessions: users background the app, kill it, or lose signal. Device variability: performance and memory vary wildly across devices.\", \"score\": 0.59089786, \"raw_content\": null}], \"response_time\": 1.04, \"request_id\": \"bf0e7eaa-a138-4b1b-a047-6cb2f80b1113\"}', name='tavily_search', id='d27acd15-7e57-4249-a8e4-d0797285685b', tool_call_id='chatcmpl-tool-a9cdc035189a5a19')]}}\n",
            "\u001b[1m[values]\u001b[0m {'messages': [HumanMessage(content='What are the top 5 mistakes teams make when instrumenting LLM apps, and how can Phoenix traces reveal each one?', additional_kwargs={}, response_metadata={}, id='063bd29e-22b9-483e-8378-16fb93e9f73f'), AIMessage(content='', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 65, 'prompt_tokens': 1822, 'total_tokens': 1887, 'completion_tokens_details': None, 'prompt_tokens_details': None}, 'model_provider': 'openai', 'model_name': 'Qwen/Qwen3-Coder-Next-FP8', 'system_fingerprint': None, 'id': 'chatcmpl-ad7f06cd56fcf084', 'finish_reason': 'tool_calls', 'logprobs': None}, id='lc_run--019c399b-59f3-7b00-b89d-cfe552369274-0', tool_calls=[{'name': 'tavily_search', 'args': {'query': 'top 5 mistakes teams make when instrumenting LLM apps'}, 'id': 'chatcmpl-tool-a9cdc035189a5a19', 'type': 'tool_call'}, {'name': 'tavily_search', 'args': {'query': 'how Phoenix traces reveal LLM app instrumentation mistakes'}, 'id': 'chatcmpl-tool-915c8c4fe212767b', 'type': 'tool_call'}], invalid_tool_calls=[], usage_metadata={'input_tokens': 1822, 'output_tokens': 65, 'total_tokens': 1887, 'input_token_details': {}, 'output_token_details': {}}), ToolMessage(content='{\"query\": \"top 5 mistakes teams make when instrumenting LLM apps\", \"follow_up_questions\": null, \"answer\": null, \"images\": [], \"results\": [{\"url\": \"https://www.linkedin.com/pulse/8-most-common-mistakes-building-llm-applications-2024-guy-korland-jmz6f\", \"title\": \"The 8 Most-Common Mistakes in Building LLM Applications in 2024 ...\", \"content\": \"Many companies built LLM applications that relied exclusively on the model\\'s pre-trained knowledge, ignoring the fundamental limitations of this approach. I observed numerous teams struggle with fundamental RAG components: poorly designed chunking strategies that fragmented context, embedding models mismatched to domain-specific content, and retrieval systems that couldn\\'t distinguish between semantically similar but contextually irrelevant information. The organizations that truly excelled built RAG systems capable of adapting retrieval strategies based on query types, incorporating human feedback to improve retrieval quality over time, and implementing proper citation mechanisms to maintain provenance and traceability of information. Many developers implemented popular LLM patterns and architectures without evaluating their fit for specific business needs. Successful organizations took time to understand the fundamental capabilities and limitations of different architectural patterns, selecting and adapting approaches based on their specific use cases rather than chasing trending GitHub repositories. The most successful implementations carefully mapped how LLM capabilities would augment existing processes, identifying clear handoff points between AI and human operators, and ensuring seamless data flow between LLM applications and other enterprise systems.\", \"score\": 0.7125728, \"raw_content\": null}, {\"url\": \"https://medium.com/@yadav.navya1601/what-i-learned-watching-teams-debug-llms-in-production-f9f959f574a6\", \"title\": \"What I Learned Watching Teams Debug LLMs in Production - Medium\", \"content\": \"Mistake #3: Black Box Failures. Here\\'s a scenario I see constantly: a user reports “the AI gave me a wrong answer.” The team checks the logs.\", \"score\": 0.68697983, \"raw_content\": null}, {\"url\": \"https://leobit.com/blog/5-common-mistakes-with-llms/\", \"title\": \"5 Common Mistakes with LLMs and Their Impact on Business - Leobit\", \"content\": \"A major strategic mistake companies make is treating AI solely as a technical product to build, rather than a core skill to develop across your\", \"score\": 0.65966886, \"raw_content\": null}, {\"url\": \"https://www.reddit.com/r/AI_Agents/comments/1nax7v1/one_year_as_an_ai_engineer_the_5_biggest/\", \"title\": \"The 5 biggest misconceptions about LLM reliability I\\'ve encountered\", \"content\": \"The bottom line: LLM reliability is a systems engineering problem, not just a model problem. You need proper observability, robust evaluation\", \"score\": 0.6255073, \"raw_content\": null}, {\"url\": \"https://aijourn.com/llm-observability-for-mobile-apps-what-to-instrument-without-violating-privacy/\", \"title\": \"LLM Observability For Mobile Apps: What To Instrument Without ...\", \"content\": \"Unreliable sessions: users background the app, kill it, or lose signal. Device variability: performance and memory vary wildly across devices.\", \"score\": 0.59089786, \"raw_content\": null}], \"response_time\": 1.04, \"request_id\": \"bf0e7eaa-a138-4b1b-a047-6cb2f80b1113\"}', name='tavily_search', id='d27acd15-7e57-4249-a8e4-d0797285685b', tool_call_id='chatcmpl-tool-a9cdc035189a5a19'), ToolMessage(content='{\"query\": \"how Phoenix traces reveal LLM app instrumentation mistakes\", \"follow_up_questions\": null, \"answer\": null, \"images\": [], \"results\": [{\"url\": \"https://www.linkedin.com/posts/mikeldking_today-arize-ai-phoenix-is-launching-llm-traces-activity-7114625085629501440-5Y0w\", \"title\": \"Today Arize AI Phoenix is launching LLM Traces and Observability ...\", \"content\": \"Today Arize AI Phoenix is launching LLM Traces and Observability to help us understand complex LLM applications and to prompt us to ask questions about these system\\'s inner workings. In order to be able to ask those questions of a system, LLM applications must be properly instrumented and must emit signals in the form of traces. Phoenix now ships with two critical features of LLM Observability: - LLM Traces - observability instrumentation for the two LLM frameworks LlamaIndex and LangChain - LLM Evals - a benchmarked set of LLM-powered evaluations for uncovering hard to identify problems such as hallucinations and retrieval relevancy arize-phoenix is entirely open-source, built on open standards, and runs entirely in the privacy of your python notebook. Let me show you how to troubleshoot an LLM application using Phoenix tracing. You can see that the LLM application is answering questions about documentation provided by arise. Phoenix tracing gives you visibility into the individual steps of your LLM application, helping you identify, troubleshoot, evaluate, and ultimately improving your system.\", \"score\": 0.78886026, \"raw_content\": null}, {\"url\": \"https://www.youtube.com/watch?v=j5WwaknZVDY\", \"title\": \"Understanding Tracing and Instrumentation with Arize Phoenix\", \"content\": \"Understanding Tracing and Instrumentation with Arize Phoenix\\\\nData Science Dojo\\\\n119000 subscribers\\\\n12 likes\\\\n2156 views\\\\n17 May 2025\\\\n🔍 Instrumenting and Tracing LLM Agents with Arize Phoenix | Full Walkthrough & What-If Analysis\\\\n\\\\nIn this video, we take a deep dive into instrumentation, tracing, and evaluation of LLM-powered applications using Arize Phoenix. Learn how spans and traces work behind the scenes, how to manually and automatically instrument your code, and how Phoenix visualizes your agent\\'s logic in real time.\\\\n\\\\n🎯 What You\\'ll Learn:\\\\n\\\\nWhat is instrumentation and why it matters for observability\\\\n\\\\nUnderstanding spans, traces, and how they map to LLM workflows\\\\n\\\\nManual vs. auto-instrumentation techniques\\\\n\\\\nExploring JSON span data inside Phoenix\\\\n\\\\nHow to use What-If Analysis to compare model responses side-by-side\\\\n\\\\nTroubleshooting Phoenix onboarding issues\\\\n\\\\nWhether you\\'re using OpenAI, LangChain, or Anthropic, see how Phoenix makes it easier to debug, evaluate, and optimize your agents—without the guesswork.\\\\n\\\\n💡 Don\\'t forget to like, subscribe, and drop a comment if you want a hands-on demo or walkthrough of a specific part of Phoenix!\\\\n\\\\n\", \"score\": 0.60274047, \"raw_content\": null}, {\"url\": \"https://rajeevbarnwal.medium.com/debugging-and-tracing-llms-like-a-pro-b560ded19fd9\", \"title\": \"Debugging and Tracing LLMs Like a Pro - Rajeev Barnwal\", \"content\": \"As a seasoned AI Leader with over a decade of experience, I have spent years building AI systems where a single user request flows through prompts, chains, tools, retrievers, and external APIs. Traditional methods often fall short, leaving developers sifting through logs or guessing at issues. That is why I am excited to share my insights on **Phoenix** by **Arize AI**, a game changing open source tool that has transformed how I monitor and trace **LLM** pipelines. In this article, I will walk you through what **Phoenix** offers, why it stands out in the 2025 landscape of **LLM** observability tools, and a step by step integration with **LangChain**. **Phoenix** is an open source observability and debugging tool tailored for large language model applications. Step 2: Register Phoenix with OpenTelemetry and Instrument LangChain. ## Wrapping Up. In my experience, Arize Phoenix makes it incredibly easy to debug, trace, and monitor LLM applications.\", \"score\": 0.5809471, \"raw_content\": null}, {\"url\": \"https://arize.com/docs/phoenix/tracing/tutorial\", \"title\": \"Tracing Tutorial - Phoenix - Arize AI\", \"content\": \"##### Tracing. * Chapter 1: Your First Traces. # Tracing Tutorial. Build a fully observable AI agent from scratch - trace every operation, measure quality, and debug conversations. For AI applications, this means capturing every LLM call, tool execution, retrieval operation, and generation - along with their inputs, outputs, latency, and token usage. Phoenix provides the infrastructure for AI observability: **tracing** to capture execution flow, **annotations** to measure quality, and **sessions** to track conversations. As you build each feature, you’ll add the corresponding observability layer - so you can see exactly how classification decisions are made, why certain documents are retrieved, and whether conversations maintain context across turns. # \\u200b Chapter 1: Your First Traces. * Trace LLM calls, tool executions, and RAG retrievals automatically. By the end of this tutorial, you’ll have a working agent with complete observability - the same patterns used by teams running AI applications in production. **Start with Chapter 1: Your First Traces →**.\", \"score\": 0.52232456, \"raw_content\": null}, {\"url\": \"https://mohdzain.com/blogs/tracing-agentic-llm-workflows-arize-phoenix-langgraph\", \"title\": \"Tracing Agentic LLMs with Arize Phoenix & LangGraph - Mohd Zain\", \"content\": \"### Tracing Agentic LLM Workflows Locally Using Arize Phoenix and LangGraph. In this post, I’ll show how I traced a **LangGraph** based agent system locally using **Arize Phoenix**, powered by **OpenTelemetry**, and how I wrapped custom agent spans to get meaningful insights beyond default LLM traces. #### What Is LLM Tracing and Why It Matters for Agentic Systems. Agentic LLM systems behave like distributed systems, making OpenTelemetry a natural fit for observability. #### Running Arize Phoenix Locally for LLM Tracing. Wraps each LangGraph node with a custom OpenTelemetry span, allowing agent-level visibility into execution flow, routing decisions, and token usage without modifying business logic. Demonstrates how the tracing setup and agent decorator are applied directly to LangGraph nodes, ensuring each agent execution is captured as a first-class trace span. Once the agentic workflow runs, navigate to the Phoenix UI at `http://localhost:6006` to visualize the traces. You’ll see a hierarchical trace graph showing agent spans, LLM calls, tool invocations, and custom attributes like token usage and routing decisions.\", \"score\": 0.39480802, \"raw_content\": null}], \"response_time\": 0.82, \"request_id\": \"a5cd669f-54c2-4c06-a148-ce1d9a9e4204\"}', name='tavily_search', id='f90b347d-3d63-402f-9642-375f495f887a', tool_call_id='chatcmpl-tool-915c8c4fe212767b')]}\n",
            "\u001b[1m[updates]\u001b[0m {'model': {'messages': [AIMessage(content='Based on the search results, here are the **top 5 mistakes teams make when instrumenting LLM apps**, and how **Arize AI Phoenix traces** can help reveal and address each one:\\n\\n---\\n\\n### 1. **Lack of End-to-End Visibility (Black Box Failures)**  \\n**Mistake**: Teams often treat LLM apps as black boxes, only logging high-level inputs/outputs and missing intermediate steps (e.g., retrieval, tool calls, prompt transformations).  \\n**How Phoenix Helps**:  \\n- Phoenix captures **full traces** of every operation—LLM calls, retrievals, tool invocations—with inputs, outputs, latency, and token usage.  \\n- It visualizes the execution flow as a trace graph, making it easy to pinpoint where failures or delays occur.  \\n- *Example*: When a user reports “wrong answer,” Phoenix traces can show whether the issue was in retrieval, prompt engineering, or the LLM itself.\\n\\n---\\n\\n### 2. **Poor RAG Instrumentation (Chunking, Embeddings, Retrieval)**  \\n**Mistake**: Teams often implement RAG without instrumenting core components—chunking strategies, embedding models, and retrieval logic—leading to irrelevant or hallucinated responses.  \\n**How Phoenix Helps**:  \\n- Phoenix traces retrieval steps, including document relevance scores and similarity metrics.  \\n- It can highlight mismatches between query intent and retrieved content, exposing flawed chunking or embedding choices.  \\n- *Example*: If retrieved documents are irrelevant, Phoenix traces can reveal whether the issue is due to poor chunking or an embedding model mismatch.\\n\\n---\\n\\n### 3. **Ignoring Latency and Token Usage Metrics**  \\n**Mistake**: Teams optimize for accuracy but overlook performance bottlenecks like excessive LLM calls or high token usage, leading to slow or costly apps.  \\n**How Phoenix Helps**:  \\n- Phoenix automatically captures **latency** and **token usage** for every span in the trace.  \\n- It enables teams to identify inefficient patterns (e.g., redundant LLM calls, unnecessary tool invocations).  \\n- *Example*: A trace might reveal that a single user request triggers 5 LLM calls, prompting optimization of the agent’s decision logic.\\n\\n---\\n\\n### 4. **Missing Contextual Session Tracking**  \\n**Mistake**: Teams fail to track multi-turn conversations, making it hard to debug context drift or inconsistent responses.  \\n**How Phoenix Helps**:  \\n- Phoenix organizes traces into **sessions**, preserving conversation history and context across turns.  \\n- It enables teams to see how earlier messages influence later responses, exposing context loss or drift.  \\n- *Example*: If a user gets contradictory answers in a chat, Phoenix sessions can show whether the agent lost earlier instructions.\\n\\n---\\n\\n### 5. **Over-Reliance on Trending Patterns Without Evaluation**  \\n**Mistake**: Teams adopt popular LLM architectures (e.g., agentic workflows, chain-of-thought) without evaluating their fit for specific use cases.  \\n**How Phoenix Helps**:  \\n- Phoenix includes **LLM Evals** (built-in benchmarks) to measure hallucinations, retrieval relevance, and faithfulness.  \\n- It allows teams to run “What-If” analyses, comparing model responses across different prompts or architectures.  \\n- *Example*: A trace might reveal that a complex agentic workflow introduces unnecessary steps, prompting simplification.\\n\\n---\\n\\n### Summary  \\nPhoenix traces transform LLM observability by providing **structured, hierarchical visibility** into every step of an LLM app’s execution. By capturing inputs, outputs, latency, and metadata, Phoenix helps teams move from reactive debugging to proactive optimization.\\n\\nFor more details, check out the [Phoenix Tracing Tutorial](https://arize.com/docs/phoenix/tracing/tutorial) or the [LangGraph + Phoenix integration guide](https://mohdzain.com/blogs/tracing-agentic-llm-workflows-arize-phoenix-langgraph).', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 802, 'prompt_tokens': 4327, 'total_tokens': 5129, 'completion_tokens_details': None, 'prompt_tokens_details': None}, 'model_provider': 'openai', 'model_name': 'Qwen/Qwen3-Coder-Next-FP8', 'system_fingerprint': None, 'id': 'chatcmpl-a6fb22af68d069d4', 'finish_reason': 'stop', 'logprobs': None}, id='lc_run--019c399b-6b89-71d0-bc5b-2fd6c2facd08-0', tool_calls=[], invalid_tool_calls=[], usage_metadata={'input_tokens': 4327, 'output_tokens': 802, 'total_tokens': 5129, 'input_token_details': {}, 'output_token_details': {}})]}}\n",
            "\u001b[1m[values]\u001b[0m {'messages': [HumanMessage(content='What are the top 5 mistakes teams make when instrumenting LLM apps, and how can Phoenix traces reveal each one?', additional_kwargs={}, response_metadata={}, id='063bd29e-22b9-483e-8378-16fb93e9f73f'), AIMessage(content='', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 65, 'prompt_tokens': 1822, 'total_tokens': 1887, 'completion_tokens_details': None, 'prompt_tokens_details': None}, 'model_provider': 'openai', 'model_name': 'Qwen/Qwen3-Coder-Next-FP8', 'system_fingerprint': None, 'id': 'chatcmpl-ad7f06cd56fcf084', 'finish_reason': 'tool_calls', 'logprobs': None}, id='lc_run--019c399b-59f3-7b00-b89d-cfe552369274-0', tool_calls=[{'name': 'tavily_search', 'args': {'query': 'top 5 mistakes teams make when instrumenting LLM apps'}, 'id': 'chatcmpl-tool-a9cdc035189a5a19', 'type': 'tool_call'}, {'name': 'tavily_search', 'args': {'query': 'how Phoenix traces reveal LLM app instrumentation mistakes'}, 'id': 'chatcmpl-tool-915c8c4fe212767b', 'type': 'tool_call'}], invalid_tool_calls=[], usage_metadata={'input_tokens': 1822, 'output_tokens': 65, 'total_tokens': 1887, 'input_token_details': {}, 'output_token_details': {}}), ToolMessage(content='{\"query\": \"top 5 mistakes teams make when instrumenting LLM apps\", \"follow_up_questions\": null, \"answer\": null, \"images\": [], \"results\": [{\"url\": \"https://www.linkedin.com/pulse/8-most-common-mistakes-building-llm-applications-2024-guy-korland-jmz6f\", \"title\": \"The 8 Most-Common Mistakes in Building LLM Applications in 2024 ...\", \"content\": \"Many companies built LLM applications that relied exclusively on the model\\'s pre-trained knowledge, ignoring the fundamental limitations of this approach. I observed numerous teams struggle with fundamental RAG components: poorly designed chunking strategies that fragmented context, embedding models mismatched to domain-specific content, and retrieval systems that couldn\\'t distinguish between semantically similar but contextually irrelevant information. The organizations that truly excelled built RAG systems capable of adapting retrieval strategies based on query types, incorporating human feedback to improve retrieval quality over time, and implementing proper citation mechanisms to maintain provenance and traceability of information. Many developers implemented popular LLM patterns and architectures without evaluating their fit for specific business needs. Successful organizations took time to understand the fundamental capabilities and limitations of different architectural patterns, selecting and adapting approaches based on their specific use cases rather than chasing trending GitHub repositories. The most successful implementations carefully mapped how LLM capabilities would augment existing processes, identifying clear handoff points between AI and human operators, and ensuring seamless data flow between LLM applications and other enterprise systems.\", \"score\": 0.7125728, \"raw_content\": null}, {\"url\": \"https://medium.com/@yadav.navya1601/what-i-learned-watching-teams-debug-llms-in-production-f9f959f574a6\", \"title\": \"What I Learned Watching Teams Debug LLMs in Production - Medium\", \"content\": \"Mistake #3: Black Box Failures. Here\\'s a scenario I see constantly: a user reports “the AI gave me a wrong answer.” The team checks the logs.\", \"score\": 0.68697983, \"raw_content\": null}, {\"url\": \"https://leobit.com/blog/5-common-mistakes-with-llms/\", \"title\": \"5 Common Mistakes with LLMs and Their Impact on Business - Leobit\", \"content\": \"A major strategic mistake companies make is treating AI solely as a technical product to build, rather than a core skill to develop across your\", \"score\": 0.65966886, \"raw_content\": null}, {\"url\": \"https://www.reddit.com/r/AI_Agents/comments/1nax7v1/one_year_as_an_ai_engineer_the_5_biggest/\", \"title\": \"The 5 biggest misconceptions about LLM reliability I\\'ve encountered\", \"content\": \"The bottom line: LLM reliability is a systems engineering problem, not just a model problem. You need proper observability, robust evaluation\", \"score\": 0.6255073, \"raw_content\": null}, {\"url\": \"https://aijourn.com/llm-observability-for-mobile-apps-what-to-instrument-without-violating-privacy/\", \"title\": \"LLM Observability For Mobile Apps: What To Instrument Without ...\", \"content\": \"Unreliable sessions: users background the app, kill it, or lose signal. Device variability: performance and memory vary wildly across devices.\", \"score\": 0.59089786, \"raw_content\": null}], \"response_time\": 1.04, \"request_id\": \"bf0e7eaa-a138-4b1b-a047-6cb2f80b1113\"}', name='tavily_search', id='d27acd15-7e57-4249-a8e4-d0797285685b', tool_call_id='chatcmpl-tool-a9cdc035189a5a19'), ToolMessage(content='{\"query\": \"how Phoenix traces reveal LLM app instrumentation mistakes\", \"follow_up_questions\": null, \"answer\": null, \"images\": [], \"results\": [{\"url\": \"https://www.linkedin.com/posts/mikeldking_today-arize-ai-phoenix-is-launching-llm-traces-activity-7114625085629501440-5Y0w\", \"title\": \"Today Arize AI Phoenix is launching LLM Traces and Observability ...\", \"content\": \"Today Arize AI Phoenix is launching LLM Traces and Observability to help us understand complex LLM applications and to prompt us to ask questions about these system\\'s inner workings. In order to be able to ask those questions of a system, LLM applications must be properly instrumented and must emit signals in the form of traces. Phoenix now ships with two critical features of LLM Observability: - LLM Traces - observability instrumentation for the two LLM frameworks LlamaIndex and LangChain - LLM Evals - a benchmarked set of LLM-powered evaluations for uncovering hard to identify problems such as hallucinations and retrieval relevancy arize-phoenix is entirely open-source, built on open standards, and runs entirely in the privacy of your python notebook. Let me show you how to troubleshoot an LLM application using Phoenix tracing. You can see that the LLM application is answering questions about documentation provided by arise. Phoenix tracing gives you visibility into the individual steps of your LLM application, helping you identify, troubleshoot, evaluate, and ultimately improving your system.\", \"score\": 0.78886026, \"raw_content\": null}, {\"url\": \"https://www.youtube.com/watch?v=j5WwaknZVDY\", \"title\": \"Understanding Tracing and Instrumentation with Arize Phoenix\", \"content\": \"Understanding Tracing and Instrumentation with Arize Phoenix\\\\nData Science Dojo\\\\n119000 subscribers\\\\n12 likes\\\\n2156 views\\\\n17 May 2025\\\\n🔍 Instrumenting and Tracing LLM Agents with Arize Phoenix | Full Walkthrough & What-If Analysis\\\\n\\\\nIn this video, we take a deep dive into instrumentation, tracing, and evaluation of LLM-powered applications using Arize Phoenix. Learn how spans and traces work behind the scenes, how to manually and automatically instrument your code, and how Phoenix visualizes your agent\\'s logic in real time.\\\\n\\\\n🎯 What You\\'ll Learn:\\\\n\\\\nWhat is instrumentation and why it matters for observability\\\\n\\\\nUnderstanding spans, traces, and how they map to LLM workflows\\\\n\\\\nManual vs. auto-instrumentation techniques\\\\n\\\\nExploring JSON span data inside Phoenix\\\\n\\\\nHow to use What-If Analysis to compare model responses side-by-side\\\\n\\\\nTroubleshooting Phoenix onboarding issues\\\\n\\\\nWhether you\\'re using OpenAI, LangChain, or Anthropic, see how Phoenix makes it easier to debug, evaluate, and optimize your agents—without the guesswork.\\\\n\\\\n💡 Don\\'t forget to like, subscribe, and drop a comment if you want a hands-on demo or walkthrough of a specific part of Phoenix!\\\\n\\\\n\", \"score\": 0.60274047, \"raw_content\": null}, {\"url\": \"https://rajeevbarnwal.medium.com/debugging-and-tracing-llms-like-a-pro-b560ded19fd9\", \"title\": \"Debugging and Tracing LLMs Like a Pro - Rajeev Barnwal\", \"content\": \"As a seasoned AI Leader with over a decade of experience, I have spent years building AI systems where a single user request flows through prompts, chains, tools, retrievers, and external APIs. Traditional methods often fall short, leaving developers sifting through logs or guessing at issues. That is why I am excited to share my insights on **Phoenix** by **Arize AI**, a game changing open source tool that has transformed how I monitor and trace **LLM** pipelines. In this article, I will walk you through what **Phoenix** offers, why it stands out in the 2025 landscape of **LLM** observability tools, and a step by step integration with **LangChain**. **Phoenix** is an open source observability and debugging tool tailored for large language model applications. Step 2: Register Phoenix with OpenTelemetry and Instrument LangChain. ## Wrapping Up. In my experience, Arize Phoenix makes it incredibly easy to debug, trace, and monitor LLM applications.\", \"score\": 0.5809471, \"raw_content\": null}, {\"url\": \"https://arize.com/docs/phoenix/tracing/tutorial\", \"title\": \"Tracing Tutorial - Phoenix - Arize AI\", \"content\": \"##### Tracing. * Chapter 1: Your First Traces. # Tracing Tutorial. Build a fully observable AI agent from scratch - trace every operation, measure quality, and debug conversations. For AI applications, this means capturing every LLM call, tool execution, retrieval operation, and generation - along with their inputs, outputs, latency, and token usage. Phoenix provides the infrastructure for AI observability: **tracing** to capture execution flow, **annotations** to measure quality, and **sessions** to track conversations. As you build each feature, you’ll add the corresponding observability layer - so you can see exactly how classification decisions are made, why certain documents are retrieved, and whether conversations maintain context across turns. # \\u200b Chapter 1: Your First Traces. * Trace LLM calls, tool executions, and RAG retrievals automatically. By the end of this tutorial, you’ll have a working agent with complete observability - the same patterns used by teams running AI applications in production. **Start with Chapter 1: Your First Traces →**.\", \"score\": 0.52232456, \"raw_content\": null}, {\"url\": \"https://mohdzain.com/blogs/tracing-agentic-llm-workflows-arize-phoenix-langgraph\", \"title\": \"Tracing Agentic LLMs with Arize Phoenix & LangGraph - Mohd Zain\", \"content\": \"### Tracing Agentic LLM Workflows Locally Using Arize Phoenix and LangGraph. In this post, I’ll show how I traced a **LangGraph** based agent system locally using **Arize Phoenix**, powered by **OpenTelemetry**, and how I wrapped custom agent spans to get meaningful insights beyond default LLM traces. #### What Is LLM Tracing and Why It Matters for Agentic Systems. Agentic LLM systems behave like distributed systems, making OpenTelemetry a natural fit for observability. #### Running Arize Phoenix Locally for LLM Tracing. Wraps each LangGraph node with a custom OpenTelemetry span, allowing agent-level visibility into execution flow, routing decisions, and token usage without modifying business logic. Demonstrates how the tracing setup and agent decorator are applied directly to LangGraph nodes, ensuring each agent execution is captured as a first-class trace span. Once the agentic workflow runs, navigate to the Phoenix UI at `http://localhost:6006` to visualize the traces. You’ll see a hierarchical trace graph showing agent spans, LLM calls, tool invocations, and custom attributes like token usage and routing decisions.\", \"score\": 0.39480802, \"raw_content\": null}], \"response_time\": 0.82, \"request_id\": \"a5cd669f-54c2-4c06-a148-ce1d9a9e4204\"}', name='tavily_search', id='f90b347d-3d63-402f-9642-375f495f887a', tool_call_id='chatcmpl-tool-915c8c4fe212767b'), AIMessage(content='Based on the search results, here are the **top 5 mistakes teams make when instrumenting LLM apps**, and how **Arize AI Phoenix traces** can help reveal and address each one:\\n\\n---\\n\\n### 1. **Lack of End-to-End Visibility (Black Box Failures)**  \\n**Mistake**: Teams often treat LLM apps as black boxes, only logging high-level inputs/outputs and missing intermediate steps (e.g., retrieval, tool calls, prompt transformations).  \\n**How Phoenix Helps**:  \\n- Phoenix captures **full traces** of every operation—LLM calls, retrievals, tool invocations—with inputs, outputs, latency, and token usage.  \\n- It visualizes the execution flow as a trace graph, making it easy to pinpoint where failures or delays occur.  \\n- *Example*: When a user reports “wrong answer,” Phoenix traces can show whether the issue was in retrieval, prompt engineering, or the LLM itself.\\n\\n---\\n\\n### 2. **Poor RAG Instrumentation (Chunking, Embeddings, Retrieval)**  \\n**Mistake**: Teams often implement RAG without instrumenting core components—chunking strategies, embedding models, and retrieval logic—leading to irrelevant or hallucinated responses.  \\n**How Phoenix Helps**:  \\n- Phoenix traces retrieval steps, including document relevance scores and similarity metrics.  \\n- It can highlight mismatches between query intent and retrieved content, exposing flawed chunking or embedding choices.  \\n- *Example*: If retrieved documents are irrelevant, Phoenix traces can reveal whether the issue is due to poor chunking or an embedding model mismatch.\\n\\n---\\n\\n### 3. **Ignoring Latency and Token Usage Metrics**  \\n**Mistake**: Teams optimize for accuracy but overlook performance bottlenecks like excessive LLM calls or high token usage, leading to slow or costly apps.  \\n**How Phoenix Helps**:  \\n- Phoenix automatically captures **latency** and **token usage** for every span in the trace.  \\n- It enables teams to identify inefficient patterns (e.g., redundant LLM calls, unnecessary tool invocations).  \\n- *Example*: A trace might reveal that a single user request triggers 5 LLM calls, prompting optimization of the agent’s decision logic.\\n\\n---\\n\\n### 4. **Missing Contextual Session Tracking**  \\n**Mistake**: Teams fail to track multi-turn conversations, making it hard to debug context drift or inconsistent responses.  \\n**How Phoenix Helps**:  \\n- Phoenix organizes traces into **sessions**, preserving conversation history and context across turns.  \\n- It enables teams to see how earlier messages influence later responses, exposing context loss or drift.  \\n- *Example*: If a user gets contradictory answers in a chat, Phoenix sessions can show whether the agent lost earlier instructions.\\n\\n---\\n\\n### 5. **Over-Reliance on Trending Patterns Without Evaluation**  \\n**Mistake**: Teams adopt popular LLM architectures (e.g., agentic workflows, chain-of-thought) without evaluating their fit for specific use cases.  \\n**How Phoenix Helps**:  \\n- Phoenix includes **LLM Evals** (built-in benchmarks) to measure hallucinations, retrieval relevance, and faithfulness.  \\n- It allows teams to run “What-If” analyses, comparing model responses across different prompts or architectures.  \\n- *Example*: A trace might reveal that a complex agentic workflow introduces unnecessary steps, prompting simplification.\\n\\n---\\n\\n### Summary  \\nPhoenix traces transform LLM observability by providing **structured, hierarchical visibility** into every step of an LLM app’s execution. By capturing inputs, outputs, latency, and metadata, Phoenix helps teams move from reactive debugging to proactive optimization.\\n\\nFor more details, check out the [Phoenix Tracing Tutorial](https://arize.com/docs/phoenix/tracing/tutorial) or the [LangGraph + Phoenix integration guide](https://mohdzain.com/blogs/tracing-agentic-llm-workflows-arize-phoenix-langgraph).', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 802, 'prompt_tokens': 4327, 'total_tokens': 5129, 'completion_tokens_details': None, 'prompt_tokens_details': None}, 'model_provider': 'openai', 'model_name': 'Qwen/Qwen3-Coder-Next-FP8', 'system_fingerprint': None, 'id': 'chatcmpl-a6fb22af68d069d4', 'finish_reason': 'stop', 'logprobs': None}, id='lc_run--019c399b-6b89-71d0-bc5b-2fd6c2facd08-0', tool_calls=[], invalid_tool_calls=[], usage_metadata={'input_tokens': 4327, 'output_tokens': 802, 'total_tokens': 5129, 'input_token_details': {}, 'output_token_details': {}})]}\n",
            "\n",
            "=== Agent Response ===\n",
            "Based on the search results, here are the **top 5 mistakes teams make when instrumenting LLM apps**, and how **Arize AI Phoenix traces** can help reveal and address each one:\n",
            "\n",
            "---\n",
            "\n",
            "### 1. **Lack of End-to-End Visibility (Black Box Failures)**  \n",
            "**Mistake**: Teams often treat LLM apps as black boxes, only logging high-level inputs/outputs and missing intermediate steps (e.g., retrieval, tool calls, prompt transformations).  \n",
            "**How Phoenix Helps**:  \n",
            "- Phoenix captures **full traces** of every operation—LLM calls, retrievals, tool invocations—with inputs, outputs, latency, and token usage.  \n",
            "- It visualizes the execution flow as a trace graph, making it easy to pinpoint where failures or delays occur.  \n",
            "- *Example*: When a user reports “wrong answer,” Phoenix traces can show whether the issue was in retrieval, prompt engineering, or the LLM itself.\n",
            "\n",
            "---\n",
            "\n",
            "### 2. **Poor RAG Instrumentation (Chunking, Embeddings, Retrieval)**  \n",
            "**Mistake**: Teams often implement RAG without instrumenting core components—chunking strategies, embedding models, and retrieval logic—leading to irrelevant or hallucinated responses.  \n",
            "**How Phoenix Helps**:  \n",
            "- Phoenix traces retrieval steps, including document relevance scores and similarity metrics.  \n",
            "- It can highlight mismatches between query intent and retrieved content, exposing flawed chunking or embedding choices.  \n",
            "- *Example*: If retrieved documents are irrelevant, Phoenix traces can reveal whether the issue is due to poor chunking or an embedding model mismatch.\n",
            "\n",
            "---\n",
            "\n",
            "### 3. **Ignoring Latency and Token Usage Metrics**  \n",
            "**Mistake**: Teams optimize for accuracy but overlook performance bottlenecks like excessive LLM calls or high token usage, leading to slow or costly apps.  \n",
            "**How Phoenix Helps**:  \n",
            "- Phoenix automatically captures **latency** and **token usage** for every span in the trace.  \n",
            "- It enables teams to identify inefficient patterns (e.g., redundant LLM calls, unnecessary tool invocations).  \n",
            "- *Example*: A trace might reveal that a single user request triggers 5 LLM calls, prompting optimization of the agent’s decision logic.\n",
            "\n",
            "---\n",
            "\n",
            "### 4. **Missing Contextual Session Tracking**  \n",
            "**Mistake**: Teams fail to track multi-turn conversations, making it hard to debug context drift or inconsistent responses.  \n",
            "**How Phoenix Helps**:  \n",
            "- Phoenix organizes traces into **sessions**, preserving conversation history and context across turns.  \n",
            "- It enables teams to see how earlier messages influence later responses, exposing context loss or drift.  \n",
            "- *Example*: If a user gets contradictory answers in a chat, Phoenix sessions can show whether the agent lost earlier instructions.\n",
            "\n",
            "---\n",
            "\n",
            "### 5. **Over-Reliance on Trending Patterns Without Evaluation**  \n",
            "**Mistake**: Teams adopt popular LLM architectures (e.g., agentic workflows, chain-of-thought) without evaluating their fit for specific use cases.  \n",
            "**How Phoenix Helps**:  \n",
            "- Phoenix includes **LLM Evals** (built-in benchmarks) to measure hallucinations, retrieval relevance, and faithfulness.  \n",
            "- It allows teams to run “What-If” analyses, comparing model responses across different prompts or architectures.  \n",
            "- *Example*: A trace might reveal that a complex agentic workflow introduces unnecessary steps, prompting simplification.\n",
            "\n",
            "---\n",
            "\n",
            "### Summary  \n",
            "Phoenix traces transform LLM observability by providing **structured, hierarchical visibility** into every step of an LLM app’s execution. By capturing inputs, outputs, latency, and metadata, Phoenix helps teams move from reactive debugging to proactive optimization.\n",
            "\n",
            "For more details, check out the [Phoenix Tracing Tutorial](https://arize.com/docs/phoenix/tracing/tutorial) or the [LangGraph + Phoenix integration guide](https://mohdzain.com/blogs/tracing-agentic-llm-workflows-arize-phoenix-langgraph).\n",
            "\n",
            "Open Phoenix UI and inspect project: local-llm-agent\n",
            "Collector target: http://192.168.86.208:6006\n"
          ]
        }
      ],
      "source": [
        "# Notebook run cell\n",
        "_load_env()\n",
        "\n",
        "project_name = os.getenv(\"PHOENIX_PROJECT_NAME\", \"local-llm-agent\")\n",
        "if \"question\" not in globals():\n",
        "    question = os.getenv(\n",
        "        \"QUESTION\",\n",
        "        \"Who is the current president of the United States and what is the latest headline about them?\",\n",
        "    )\n",
        "\n",
        "setup_phoenix(project_name=project_name)\n",
        "answer = run_agent(question)\n",
        "\n",
        "print(\"\\n=== Agent Response ===\")\n",
        "print(answer)\n",
        "print(\"\\nOpen Phoenix UI and inspect project:\", project_name)\n",
        "print(\n",
        "    \"Collector target:\",\n",
        "    os.getenv(\n",
        "        \"PHOENIX_COLLECTOR_ENDPOINT\",\n",
        "        f\"http://{os.getenv('PHOENIX_HOST', 'localhost')}:{os.getenv('PHOENIX_PORT', '6006')}\",\n",
        "    ),\n",
        ")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
